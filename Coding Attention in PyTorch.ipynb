{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding Attention in PyTorch\n",
    "This notebook contains Self-Attention, Masked Self-Attention, Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dmodel=2, drow=0, dcol=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.wq = nn.Linear(dmodel, dmodel, bias=False)\n",
    "        self.wk = nn.Linear(dmodel, dmodel, bias=False)\n",
    "        self.wv = nn.Linear(dmodel, dmodel, bias=False)\n",
    "\n",
    "        self.drow = drow\n",
    "        self.dcol = dcol\n",
    "\n",
    "    def forward(self, encodingq, encodingk, encodingv, mask=None):\n",
    "        q = self.wq(encodingq)\n",
    "        k = self.wk(encodingk)\n",
    "        v = self.wv(encodingv)\n",
    "\n",
    "        sims = torch.matmul(q, k.transpose(self.drow, self.dcol)) / torch.sqrt(torch.tensor(k.size(-1)))\n",
    "\n",
    "        if mask is not None:\n",
    "            sims = torch.masked_fill(sims, mask, value=-1e9)\n",
    "\n",
    "        sims_probs = F.softmax(sims, dim=self.dcol)\n",
    "\n",
    "        output = torch.matmul(sims_probs, v)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "attention = Attention()\n",
    "\n",
    "## create a matrix of token encodings...\n",
    "encodings_matrix = torch.tensor([[1.16, 0.23],\n",
    "                                 [0.57, 1.36],\n",
    "                                 [4.41, -2.16]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0100, 1.0641],\n",
      "        [0.2040, 0.7057],\n",
      "        [3.4989, 2.2427]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output = attention(encodings_matrix, encodings_matrix, encodings_matrix)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate encoder-decoder attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder outputs shape: torch.Size([3, 2])\n",
      "Decoder states shape: torch.Size([2, 2])\n",
      "Encoder-Decoder attention output shape: torch.Size([2, 2])\n",
      "tensor([[-0.2030,  2.3877],\n",
      "        [-0.1989,  2.4166]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Encoder outputs (assume these are processed source sentence embeddings)\n",
    "encoder_outputs = torch.tensor([[1.0, 2.0],   # Encoded word 1\n",
    "                              [3.0, 4.0],     # Encoded word 2\n",
    "                              [5.0, 6.0]])    # Encoded word 3\n",
    "\n",
    "# Decoder current states (different sequence length!)\n",
    "decoder_states = torch.tensor([[0.5, 0.5], # Current decoder state 1\n",
    "                              [1.5, 1.5]])   # Current decoder state 2  \n",
    "\n",
    "# Use decoder states as queries, encoder outputs as keys and values\n",
    "enc_dec_attention_output = attention(decoder_states,      # q (from decoder)\n",
    "                                   encoder_outputs,       # k (from encoder)\n",
    "                                   encoder_outputs)       # v (from encoder)\n",
    "\n",
    "print(\"Encoder outputs shape:\", encoder_outputs.shape)\n",
    "print(\"Decoder states shape:\", decoder_states.shape)\n",
    "print(\"Encoder-Decoder attention output shape:\", enc_dec_attention_output.shape)\n",
    "print(enc_dec_attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate masked self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6038,  0.7434],\n",
      "        [-0.0062,  0.6072],\n",
      "        [ 3.4989,  2.2427]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mask = torch.tril(torch.ones((encodings_matrix.size(0),encodings_matrix.size(0))))\n",
    "mask = mask==0\n",
    "\n",
    "output = attention(encodings_matrix, encodings_matrix, encodings_matrix, mask)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, dmodel=2, drow=0, dcol=1, nhead=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.heads = nn.ModuleList([Attention(dmodel, drow, dcol) for _ in range(self.nhead)])\n",
    "\n",
    "    def forward(self, encodingq, encodingk, encodingv, mask=None):\n",
    "        output = []\n",
    "        for head in self.heads:\n",
    "            output.append(head(encodingq, encodingk, encodingv))\n",
    "        \n",
    "        output_con = torch.concat(output, dim=-1)\n",
    "\n",
    "        return output_con"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that we can correctly calculate attention with single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0100, 1.0641],\n",
      "        [0.2040, 0.7057],\n",
      "        [3.4989, 2.2427]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## set the seed for the random number generator\n",
    "torch.manual_seed(42)\n",
    "\n",
    "singlehead = MultiHeadAttention(dmodel=2, drow=0, dcol=1, nhead=1)\n",
    "output = singlehead(encodings_matrix, encodings_matrix, encodings_matrix)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate attention with multiple head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0100,  1.0641, -0.7081, -0.8268],\n",
      "        [ 0.2040,  0.7057, -0.7417, -0.9193],\n",
      "        [ 3.4989,  2.2427, -0.7190, -0.8447]], grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "multihead = MultiHeadAttention(dmodel=2, drow=0, dcol=1, nhead=2)\n",
    "print(multihead(encodings_matrix, encodings_matrix, encodings_matrix))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
