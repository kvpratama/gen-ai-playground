{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:12<00:00,  6.25s/it]\n"
     ]
    }
   ],
   "source": [
    "#Load model and tokenizer \n",
    "model = AutoModelForCausalLM.from_pretrained( \"microsoft/Phi-3-mini-4k-instruct\", device_map=\"xpu\", torch_dtype=\"auto\", trust_remote_code= True , ) \n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini4k-instruct\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|> Subject: Sincere Apologies for the Gardening Mishap\n",
      "\n",
      "\n",
      "Dear\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input prompt\n",
    "inputs_id = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"xpu\")\n",
    "\n",
    "# Generate text\n",
    "generation_output = model.generate(inputs_id, max_new_tokens=20)\n",
    "\n",
    "# Print the output\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14350   385  4876 27746  5281   304 19235   363   278 25305   293 16423\n",
      "   292   286   728   481 29889 12027  7420   920   372  9559 29889 32001]\n",
      "Write an email apolog izing to Sarah for the trag ic garden ing m ish ap . Exp lain how it happened . <|assistant|> "
     ]
    }
   ],
   "source": [
    "print(inputs_id[0].cpu().numpy())\n",
    "\n",
    "# decode inputs_id\n",
    "for id in inputs_id[0]:\n",
    "    print(tokenizer.decode(id), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14350   385  4876 27746  5281   304 19235   363   278 25305   293 16423\n",
      "   292   286   728   481 29889 12027  7420   920   372  9559 29889 32001\n",
      "  3323   622 29901   317  3742   406  6225 11763   363   278 19906   292\n",
      "   341   728   481    13    13    13 29928   799]\n",
      "Sub\n",
      "ject\n",
      "Subject\n",
      ":\n"
     ]
    }
   ],
   "source": [
    "print(generation_output[0].cpu().numpy())\n",
    "\n",
    "print(tokenizer.decode(3323))\n",
    "print(tokenizer.decode(622))\n",
    "print(tokenizer.decode([3323, 622]))\n",
    "print(tokenizer.decode(29901))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" English and CAPITALIZATION show_tokens False None elif == >= else: two tabs:\" \" Three tabs: \"   \" 12.0*50=600 \"\"\"\n",
    "\n",
    "colors_list = ['102;194;165', '252;141;98', '141;160;203', '231;138;195', '166;216;84', '255;217;47' ]\n",
    "\n",
    "def show_tokens(sentence, tokenizer_name): \n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name) \n",
    "    token_ids = tokenizer(sentence).input_ids \n",
    "    for idx, t in enumerate(token_ids): \n",
    "        print(\n",
    "            f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' + \n",
    "            tokenizer.decode(t) + \n",
    "            '\\x1b[0m', end=' ' \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98menglish\u001b[0m \u001b[0;30;48;2;141;160;203mand\u001b[0m \u001b[0;30;48;2;231;138;195mcapital\u001b[0m \u001b[0;30;48;2;166;216;84m##ization\u001b[0m \u001b[0;30;48;2;255;217;47mshow\u001b[0m \u001b[0;30;48;2;102;194;165m_\u001b[0m \u001b[0;30;48;2;252;141;98mtoken\u001b[0m \u001b[0;30;48;2;141;160;203m##s\u001b[0m \u001b[0;30;48;2;231;138;195mfalse\u001b[0m \u001b[0;30;48;2;166;216;84mnone\u001b[0m \u001b[0;30;48;2;255;217;47meli\u001b[0m \u001b[0;30;48;2;102;194;165m##f\u001b[0m \u001b[0;30;48;2;252;141;98m=\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195m>\u001b[0m \u001b[0;30;48;2;166;216;84m=\u001b[0m \u001b[0;30;48;2;255;217;47melse\u001b[0m \u001b[0;30;48;2;102;194;165m:\u001b[0m \u001b[0;30;48;2;252;141;98mtwo\u001b[0m \u001b[0;30;48;2;141;160;203mtab\u001b[0m \u001b[0;30;48;2;231;138;195m##s\u001b[0m \u001b[0;30;48;2;166;216;84m:\u001b[0m \u001b[0;30;48;2;255;217;47m\"\u001b[0m \u001b[0;30;48;2;102;194;165m\"\u001b[0m \u001b[0;30;48;2;252;141;98mthree\u001b[0m \u001b[0;30;48;2;141;160;203mtab\u001b[0m \u001b[0;30;48;2;231;138;195m##s\u001b[0m \u001b[0;30;48;2;166;216;84m:\u001b[0m \u001b[0;30;48;2;255;217;47m\"\u001b[0m \u001b[0;30;48;2;102;194;165m\"\u001b[0m \u001b[0;30;48;2;252;141;98m12\u001b[0m \u001b[0;30;48;2;141;160;203m.\u001b[0m \u001b[0;30;48;2;231;138;195m0\u001b[0m \u001b[0;30;48;2;166;216;84m*\u001b[0m \u001b[0;30;48;2;255;217;47m50\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98m600\u001b[0m \u001b[0;30;48;2;141;160;203m[SEP]\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;30;48;2;102;194;165m[CLS]\u001b[0m \u001b[0;30;48;2;252;141;98mEnglish\u001b[0m \u001b[0;30;48;2;141;160;203mand\u001b[0m \u001b[0;30;48;2;231;138;195mCA\u001b[0m \u001b[0;30;48;2;166;216;84m##PI\u001b[0m \u001b[0;30;48;2;255;217;47m##TA\u001b[0m \u001b[0;30;48;2;102;194;165m##L\u001b[0m \u001b[0;30;48;2;252;141;98m##I\u001b[0m \u001b[0;30;48;2;141;160;203m##Z\u001b[0m \u001b[0;30;48;2;231;138;195m##AT\u001b[0m \u001b[0;30;48;2;166;216;84m##ION\u001b[0m \u001b[0;30;48;2;255;217;47mshow\u001b[0m \u001b[0;30;48;2;102;194;165m_\u001b[0m \u001b[0;30;48;2;252;141;98mtoken\u001b[0m \u001b[0;30;48;2;141;160;203m##s\u001b[0m \u001b[0;30;48;2;231;138;195mF\u001b[0m \u001b[0;30;48;2;166;216;84m##als\u001b[0m \u001b[0;30;48;2;255;217;47m##e\u001b[0m \u001b[0;30;48;2;102;194;165mNone\u001b[0m \u001b[0;30;48;2;252;141;98mel\u001b[0m \u001b[0;30;48;2;141;160;203m##if\u001b[0m \u001b[0;30;48;2;231;138;195m=\u001b[0m \u001b[0;30;48;2;166;216;84m=\u001b[0m \u001b[0;30;48;2;255;217;47m>\u001b[0m \u001b[0;30;48;2;102;194;165m=\u001b[0m \u001b[0;30;48;2;252;141;98melse\u001b[0m \u001b[0;30;48;2;141;160;203m:\u001b[0m \u001b[0;30;48;2;231;138;195mtwo\u001b[0m \u001b[0;30;48;2;166;216;84mta\u001b[0m \u001b[0;30;48;2;255;217;47m##bs\u001b[0m \u001b[0;30;48;2;102;194;165m:\u001b[0m \u001b[0;30;48;2;252;141;98m\"\u001b[0m \u001b[0;30;48;2;141;160;203m\"\u001b[0m \u001b[0;30;48;2;231;138;195mThree\u001b[0m \u001b[0;30;48;2;166;216;84mta\u001b[0m \u001b[0;30;48;2;255;217;47m##bs\u001b[0m \u001b[0;30;48;2;102;194;165m:\u001b[0m \u001b[0;30;48;2;252;141;98m\"\u001b[0m \u001b[0;30;48;2;141;160;203m\"\u001b[0m \u001b[0;30;48;2;231;138;195m12\u001b[0m \u001b[0;30;48;2;166;216;84m.\u001b[0m \u001b[0;30;48;2;255;217;47m0\u001b[0m \u001b[0;30;48;2;102;194;165m*\u001b[0m \u001b[0;30;48;2;252;141;98m50\u001b[0m \u001b[0;30;48;2;141;160;203m=\u001b[0m \u001b[0;30;48;2;231;138;195m600\u001b[0m \u001b[0;30;48;2;166;216;84m[SEP]\u001b[0m "
     ]
    }
   ],
   "source": [
    "show_tokens(text, \"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/2.9.png)\n",
    "\n",
    "In word2vec, each word is assigned a fixed (static) embedding by simply performing a table lookup. In contrast, when you use a transformer-based model like DeBERTa:\n",
    "\n",
    "- Static Embedding: The token is first mapped to a static embedding vector.\n",
    "- Positional Embedding: A positional embedding is then added to encode the token's position in the sequence.\n",
    "- Transformer Layers: The combined embedding goes through multiple transformer layers where self-attention and feed-forward networks modify it based on the context.\n",
    "\n",
    "The final output in output.last_hidden_state is a contextualized embedding vector for each token, meaning that the same token can have different representations depending on its surrounding words.\n",
    "\n",
    "This is a key difference:\n",
    "\n",
    "- word2vec: Embedding is a simple lookup from a fixed table.\n",
    "- Transformer models: Embedding is dynamically computed through several processing steps (static embedding, positional encoding, and transformer layers).\n",
    "\n",
    "Thus, the token representation from a transformer model is much richer and context-sensitive compared to the static lookup in word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n",
      "Output shape:  torch.Size([1, 4, 384])\n",
      "Tokens ID:  tensor([[    1, 42891,   232,     2]])\n",
      "Decode Tokens:\n",
      "1 : [CLS] || 42891 : hello || 232 :  world || 2 : [SEP] || \n",
      "\n",
      "hello hello world hello world\n",
      "Output shape:  torch.Size([1, 7, 384])\n",
      "Tokens ID:  tensor([[    1, 42891, 20760,   232, 20760,   232,     2]])\n",
      "Decode Tokens:\n",
      "1 : [CLS] || 42891 : hello || 20760 :  hello || 232 :  world || 20760 :  hello || 232 :  world || 2 : [SEP] || \n",
      "\n",
      "hello hello world\n",
      "Output shape:  torch.Size([1, 5, 384])\n",
      "Tokens ID:  tensor([[    1, 42891, 20760,   232,     2]])\n",
      "Decode Tokens:\n",
      "1 : [CLS] || 42891 : hello || 20760 :  hello || 232 :  world || 2 : [SEP] || \n",
      "\n",
      "world hello world\n",
      "Output shape:  torch.Size([1, 5, 384])\n",
      "Tokens ID:  tensor([[    1,  8331, 20760,   232,     2]])\n",
      "Decode Tokens:\n",
      "1 : [CLS] || 8331 : world || 20760 :  hello || 232 :  world || 2 : [SEP] || \n",
      "\n",
      "world hello\n",
      "Output shape:  torch.Size([1, 4, 384])\n",
      "Tokens ID:  tensor([[    1,  8331, 20760,     2]])\n",
      "Decode Tokens:\n",
      "1 : [CLS] || 8331 : world || 20760 :  hello || 2 : [SEP] || \n",
      "\n",
      "Hello World\n",
      "Output shape:  torch.Size([1, 4, 384])\n",
      "Tokens ID:  tensor([[    1, 31414,   623,     2]])\n",
      "Decode Tokens:\n",
      "1 : [CLS] || 31414 : Hello || 623 :  World || 2 : [SEP] || \n",
      "\n",
      "World Hello\n",
      "Output shape:  torch.Size([1, 4, 384])\n",
      "Tokens ID:  tensor([[    1, 10988, 20920,     2]])\n",
      "Decode Tokens:\n",
      "1 : [CLS] || 10988 : World || 20920 :  Hello || 2 : [SEP] || \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\", trust_remote_code=True)\n",
    "\n",
    "def analyze_text(input_text, tokenizer, model):\n",
    "    print(input_text)\n",
    "    # Break down the text into smaller pieces (tokens) (word, subword, or character)\n",
    "    tokens = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Pass the tokens to the model\n",
    "    output = model(**tokens)\n",
    "\n",
    "    print(\"Output shape: \", output.last_hidden_state.shape)\n",
    "    print(\"Tokens ID: \", tokens['input_ids'])\n",
    "    print(\"Decode Tokens:\")\n",
    "    for token in tokens[\"input_ids\"][0]:\n",
    "        print(token.item(), \":\", tokenizer.decode(token), end=\" || \")\n",
    "\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Many modern tokenizers (including those used for models like DeBERTa) are sensitive to whitespace. \n",
    "# For example, if a tokenizer uses a byte-level or subword method, it often treats a word differently \n",
    "# if it appears at the beginning of a sentence versus when it follows a space.\n",
    "analyze_text(\"hello world\", tokenizer, model)\n",
    "analyze_text(\"hello hello world hello world\", tokenizer, model)\n",
    "analyze_text(\"hello hello world\", tokenizer, model)\n",
    "analyze_text(\"world hello world\", tokenizer, model)\n",
    "analyze_text(\"world hello\", tokenizer, model)\n",
    "analyze_text(\"Hello World\", tokenizer, model)\n",
    "analyze_text(\"World Hello\", tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer \n",
    "\n",
    "# Load model \n",
    "model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\") \n",
    "# Convert text to text embeddings \n",
    "vector = model.encode(\"Best movie ever!\")\n",
    "\n",
    "print(vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Song Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from urllib import request\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the playlist dataset file\n",
    "data = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/train.txt')\n",
    "\n",
    "# Parse the playlist dataset file. Skip the first two lines as\n",
    "# they only contain metadata\n",
    "lines = data.read().decode(\"utf-8\").split('\\n')[2:]\n",
    "\n",
    "# Remove playlists with only one song\n",
    "playlists = [s.rstrip().split() for s in lines if len(s.split())> 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load song metadata\n",
    "songs_file = request.urlopen('https://storage.googleapis.com/maps-premium/dataset/yes_complete/song_hash.txt')\n",
    "songs_file = songs_file.read().decode(\"utf-8\").split('\\n')\n",
    "songs = [s.rstrip().split('\\t') for s in songs_file]\n",
    "songs_df = pd.DataFrame(data=songs, columns = ['id', 'title', 'artist'])\n",
    "songs_df = songs_df.set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playlist #1:\n",
      "  ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '2', '42', '43', '44', '45', '46', '47', '48', '20', '49', '8', '50', '51', '52', '53', '54', '55', '56', '57', '25', '58', '59', '60', '61', '62', '3', '63', '64', '65', '66', '46', '47', '67', '2', '48', '68', '69', '70', '57', '50', '71', '72', '53', '73', '25', '74', '59', '20', '46', '75', '76', '77', '59', '20', '43'] \n",
      "\n",
      "Playlist #2:\n",
      "  ['78', '79', '80', '3', '62', '81', '14', '82', '48', '83', '84', '17', '85', '86', '87', '88', '74', '89', '90', '91', '4', '73', '62', '92', '17', '53', '59', '93', '94', '51', '50', '27', '95', '48', '96', '97', '98', '99', '100', '57', '101', '102', '25', '103', '3', '104', '105', '106', '107', '47', '108', '109', '110', '111', '112', '113', '25', '63', '62', '114', '115', '84', '116', '117', '118', '119', '120', '121', '122', '123', '50', '70', '71', '124', '17', '85', '14', '82', '48', '125', '47', '46', '72', '53', '25', '73', '4', '126', '59', '74', '20', '43', '127', '128', '129', '13', '82', '48', '130', '131', '132', '133', '134', '135', '136', '137', '59', '46', '138', '43', '20', '139', '140', '73', '57', '70', '141', '3', '1', '74', '142', '143', '144', '145', '48', '13', '25', '146', '50', '147', '126', '59', '20', '148', '149', '150', '151', '152', '56', '153', '154', '155', '156', '157', '158', '159', '160', '161', '162', '163', '164', '165', '166', '167', '168', '169', '170', '171', '172', '173', '174', '175', '60', '176', '51', '177', '178', '179', '180', '181', '182', '183', '184', '185', '57', '186', '187', '188', '189', '190', '191', '46', '192', '193', '194', '195', '196', '197', '198', '25', '199', '200', '49', '201', '100', '202', '203', '204', '205', '206', '207', '32', '208', '209', '210']\n",
      "Song in playlist  0 : \n",
      "Gucci Time (w\\/ Swizz Beatz) | Aston Martin Music (w\\/ Drake & Chrisette Michelle) | Get Back Up (w\\/ Chris Brown) | Hot Toddy (w\\/ Jay-Z & Ester Dean) | Whip My Hair |  ...\n",
      "Song in playlist  1 : \n",
      "Soca Bhangra | Fettin On (w\\/ Machel Montano) | Ants In Yuh Sugar Pan | Hot Toddy (w\\/ Jay-Z & Ester Dean) | Runaway (w\\/ Pusha T) |  ...\n",
      "Song in playlist  2 : \n",
      "Hypnotize | Southern Hospitality (w\\/ Pharrell) | I'm Ill (w\\/ Fabolous) | Monster (w\\/ Rick Ross, Jay-Z, Nicki Minaj & Bon Iver) | Right Thru Me |  ...\n"
     ]
    }
   ],
   "source": [
    "print( 'Playlist #1:\\n ', playlists[0], '\\n')\n",
    "print( 'Playlist #2:\\n ', playlists[1])\n",
    "\n",
    "# print(songs_df.head())\n",
    "# print(songs_df.iloc[0]['title'])\n",
    "# print(type(int(playlists[0][0])))\n",
    "# print(songs_df.iloc[int(playlists[0][0])])\n",
    "\n",
    "for i in range(3):\n",
    "    print('Song in playlist ', i, ': ')\n",
    "    for j in range(5):\n",
    "        print(songs_df.iloc[int(playlists[i][j])]['title'], end=\" | \")\n",
    "    print(\" ...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Train our Word2Vec model\n",
    "model = Word2Vec(\n",
    "    playlists, vector_size=32, window=20, negative=50, min_count=1, workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3119', 0.9978693723678589),\n",
       " ('2849', 0.9977297186851501),\n",
       " ('3126', 0.9970256686210632),\n",
       " ('11473', 0.9968244433403015),\n",
       " ('6626', 0.9958199262619019),\n",
       " ('10105', 0.9957699179649353),\n",
       " ('6658', 0.9956603050231934),\n",
       " ('1922', 0.9956520199775696),\n",
       " ('9995', 0.9956437945365906),\n",
       " ('10084', 0.9954245686531067)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song_id = 2172\n",
    "# Ask the model for songs similar to song #2172\n",
    "model.wv.most_similar(positive=str(song_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar song with:  I Let A Song Go Out Of My Heart  by:  Grant Stewart\n",
      "                        title                       artist\n",
      "id                                                        \n",
      "36497         Never Let Me Go                  Stacey Kent\n",
      "54919             Pharoah Joy  Joe Locke & David Hazeltine\n",
      "56020   Afro 6\\/8 Minor Blues              Larry Vuckovich\n",
      "36507              FluteVibes               Gerald Beckett\n",
      "44093            Another Star                 Cedar Walton\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def similar_song(song_id):\n",
    "    results = np.array(model.wv.most_similar(positive=str(song_id), topn=5))[:,0]\n",
    "    print(\"Similar song with: \", songs_df.iloc[int(song_id)][\"title\"], \" by: \", songs_df.iloc[int(song_id)][\"artist\"])\n",
    "    print(songs_df.iloc[results])\n",
    "\n",
    "similar_song(42231)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
