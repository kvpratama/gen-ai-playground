{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7. Advanced Text Generation Techniques and Tools\n",
    "\n",
    "![\"7.1\"](img/7.1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain import LlamaCpp\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"./Phi-3-mini-4k-instruct-fp16.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    max_tokens=500,\n",
    "    n_ctx=2048,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Create a prompt template with the \"input_prompt\" variable\n",
    "template = \"\"\"<|user|>\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_chain = prompt | llm\n",
    "\n",
    "# formatted_prompt = prompt.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})\n",
    "# output = llm.invoke(formatted_prompt)\n",
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Hello Maarten! The answer to 1 + 1 is 2.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the chain\n",
    "basic_chain.invoke(\n",
    "    {\n",
    "        \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A chain with multiple prompts\n",
    "\n",
    "![\"7.8\"](img/7.8.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "# Create a chain for the title of our story\n",
    "template = \"\"\"<|user|>\n",
    "Create a title for a story about {summary}. Only return the title.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
    "title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'a girl that lost her mother',\n",
       " 'title': ' \"The Unfading Embrace: A Journey Through Grief\"'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title.invoke({\"summary\": \"a girl that lost her mother\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain for the character description using the summary and title\n",
    "template = \"\"\"<|user|>\n",
    "Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "character_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"summary\", \"title\"]\n",
    ")\n",
    "character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain for the story using the summary, title, and character description\n",
    "template = \"\"\"<|user|>\n",
    "Create a story about {summary} with the title {title}. The main charachter is: {character}. Only return the story and it cannot be longer than one paragraph<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "story_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",
    ")\n",
    "story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'a girl that lost her mother',\n",
       " 'title': ' \"Whispers of a Lost Mother: A Journey Through Grief\"',\n",
       " 'character': \" The main character, Lily, is a resilient yet fragile young girl struggling to cope with the overwhelming grief of losing her beloved mother. Throughout the story, she embarks on an emotional journey to reconcile her pain and find solace in cherished memories and newfound strength.\\n\\nLily is depicted as a sensitive and empathetic child who yearns for her mother's comforting presence, making it challenging for her to navigate the turmoil that grief brings upon her life. As she grows older, however, Lily finds herself slowly adapting, displaying an inner strength borne from her desire to honor her late mother and keep their bond alive in every heartbeat of her life.\",\n",
       " 'story': \" Whispers of a Lost Mother: A Journey Through Grief\\n\\nLily, an empathetic and sensitive young girl, found herself engulfed in the suffocating waves of grief after losing her beloved mother. Her days were filled with memories that echoed through her heart like haunting whispers, each one a testament to their irreplaceable bond. Though fragile on the surface, Lily harbored an inner resilience as she grappled with overwhelming sorrow, seeking solace in cherished moments and the strength imparted by her mother's unwavering love. As years passed like fleeting shadows, Lily embarked on an emotional odyssey, navigating the labyrinth of loss to discover a profound acceptance that intertwined with memories, forging an indelible connection between past and present—a resilient spirit honoring her lost mother's legacy through every beat of her enduring heart.\"}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Combine all three components to create the full chain\n",
    "llm_chain = title | character | story\n",
    "\n",
    "llm_chain.invoke(\"a girl that lost her mother\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory: Helping LLMs to remember conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Hello Maarten! 1 + 1 equals 2. It's a basic arithmetic addition problem.\""
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's give the LLM our name\n",
    "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" I'm unable to determine your name without more context. Privacy guidelines prevent me from knowing personal information about individuals. If you need help with something specific, feel free to ask!\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, we ask the LLM to reproduce the name\n",
    "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an updated prompt template to include a chat history\n",
    "template = \"\"\"<|user|>Current conversation:{chat_history}\n",
    "\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IdeaPad\\AppData\\Local\\Temp\\ipykernel_31908\\1531358956.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Define the type of Memory we will use\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# Chain the LLM, Prompt, and Memory together\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\llama_cpp\\llama.py:1240: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'Hi! My name is Maarten. What is 1 + 1?',\n",
       " 'chat_history': '',\n",
       " 'text': \" The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units.\"}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a conversation and ask a basic question\n",
    "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\llama_cpp\\llama.py:1240: RuntimeWarning: Detected duplicate leading \"<s>\" in prompt, this will likely reduce response quality, consider removing it...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my name?',\n",
       " 'chat_history': \"Human: Hi! My name is Maarten. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units.\",\n",
       " 'text': ' Your name is Maarten.\\n\\nAs for the math question, 1 + 1 equals 2.'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does the LLM remember the name we gave it?\n",
    "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Windowed Conversation Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Retain only the last 2 conversations in memory\n",
    "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
    "\n",
    "# Chain the LLM, Prompt, and Memory together\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is 3 + 3?',\n",
       " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. It seems you're engaging in a friendly conversation! How can I assist you further? Perhaps with some math or other information related to your interests or age-related topics?\\n\\nRemember, it's always great to share bits of personal information, but also happy to help on any specific questions you might have.\",\n",
       " 'text': \" The answer to 3 + 3 is 6. How can I further assist you today, Maarten? Is there anything specific you'd like to know or discuss?\\n\\nRemember, I'm here to help with any questions you have!\"}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask two questions and generate two conversations in its memory\n",
    "llm_chain.invoke({\"input_prompt\":\"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\"})\n",
    "llm_chain.invoke({\"input_prompt\":\"What is 3 + 3?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my name?',\n",
       " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  The answer to 1 + 1 is 2. It seems you're engaging in a friendly conversation! How can I assist you further? Perhaps with some math or other information related to your interests or age-related topics?\\n\\nRemember, it's always great to share bits of personal information, but also happy to help on any specific questions you might have.\\nHuman: What is 3 + 3?\\nAI:  The answer to 3 + 3 is 6. How can I further assist you today, Maarten? Is there anything specific you'd like to know or discuss?\\n\\nRemember, I'm here to help with any questions you have!\",\n",
       " 'text': \" Your name is Maarten, as you mentioned at the beginning of our conversation.\\nHere's the answer to your question: 3 + 3 equals 6. If there's anything else I can help you with, feel free to ask!\"}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether it knows the name we gave it\n",
    "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my age?',\n",
       " 'chat_history': \"Human: What is 3 + 3?\\nAI:  The answer to 3 + 3 is 6. How can I further assist you today, Maarten? Is there anything specific you'd like to know or discuss?\\n\\nRemember, I'm here to help with any questions you have!\\nHuman: What is my name?\\nAI:  Your name is Maarten, as you mentioned at the beginning of our conversation.\\nHere's the answer to your question: 3 + 3 equals 6. If there's anything else I can help you with, feel free to ask!\",\n",
       " 'text': \" I'm sorry, but as an AI, I don't have access to personal data such as your age unless you provide it during our conversation. Privacy is important, so let's focus on general questions or topics you might need assistance with!\"}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether it knows the age we gave it\n",
    "llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary prompt template\n",
    "summary_prompt_template = \"\"\"<|user|>Summarize the conversations and update with the new lines.\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "new lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary:<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"new_lines\", \"summary\"],\n",
    "    template=summary_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IdeaPad\\AppData\\Local\\Temp\\ipykernel_31908\\4216441337.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# Define the type of memory we will use\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    memory_key=\"chat_history\",\n",
    "    prompt=summary_prompt\n",
    ")\n",
    "\n",
    "# Chain the LLM, prompt, and memory together\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my name?',\n",
       " 'chat_history': ' **Updated summary:**  \\n\\nIn our latest exchange, Maarten introduced himself and posed a classic arithmetic question in English - what is 1 + 1. Responding with enthusiasm and an educational twist, the AI, MathGenius3000, confirmed that the answer to this basic addition problem is indeed two. To add more flavor to our interaction, MathGenius3000 engaged in a role-play by presenting the same information using French language along with an analogy related to shoes and painting, providing both linguistic diversity and a creative approach to explaining the concept of addition.',\n",
       " 'text': ' Your name has not been mentioned in our current conversation, but you can refer to me as MathGenius3000. If you have a preferred nickname or alias for this AI, feel free to use it!'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a conversation and ask for the name\n",
    "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})\n",
    "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What was the first question I asked?',\n",
       " 'chat_history': ' **Updated Summary:**\\n\\nIn our latest conversation, the human named Maarten interacted with AI MathGenius3000 by asking a basic arithmetic question in English (\"What is 1 + 1?\") which was answered correctly as two. Additionally, MathGenius3000 demonstrated linguistic diversity and creativity by answering the same question in French using an analogy involving shoes and painting to explain addition. Further into the conversation, when asked for his name, MathGenius3000 clarified its identity as a neutral AI entity but offered flexibility for user-preferred names or nicknames.',\n",
       " 'text': ' The first question you asked was, \"What is 1 + 1?\"'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether it has summarized everything thus far\n",
    "llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': ' **Updated Summary:**\\nDuring the conversation, human Maarten inquired MathGenius3000 with a basic arithmetic question \"What is 1 + 1?\" The AI correctly provided the answer as two. It further showcased linguistic diversity and creativity by explaining addition through an analogy involving shoes and painting in French (\"Un chaussure avec une peinture, cela fait deux!\"). When asked for its name, MathGenius3000 identified itself as a neutral AI but expressed willingness to be addressed by alternative names or nicknames if preferred. Maarten also sought clarification on the initial question he posed, which prompted MathGenius3000 to reaffirm that it was \"1 + 1\".'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what the summary is thus far\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents: Creating a System of LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"7.15\"](img/7.15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"7.16\"](img/7.16.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns patterns from data to make predictions or decisions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "import os\n",
    "\n",
    "api_key = os.environ[\"GEMINI_API_KEY\"]\n",
    "client = genai.Client(api_key=api_key)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.0-flash\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI learns from data to make predictions or decisions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-1.5-flash-8b\", contents=\"Explain how AI works in a few words\"\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ[\"GEMINI_API_KEY\"]\n",
    "\n",
    "gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash-8b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ReAct template\n",
    "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=react_template,\n",
    "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, Tool\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "\n",
    "# You can create the tool to pass to an agent\n",
    "search = DuckDuckGoSearchResults()\n",
    "search_tool = Tool(\n",
    "    name=\"duckduck\",\n",
    "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
    "    func=search.run,\n",
    ")\n",
    "\n",
    "# Prepare tools\n",
    "tools = load_tools([\"llm-math\"], llm=gemini)\n",
    "tools.append(search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "# Construct the ReAct agent\n",
    "agent = create_react_agent(gemini, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mQuestion: What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?\n",
      "Thought: I need to find the current price of a MacBook Pro in USD and then convert it to EUR using the given exchange rate.\n",
      "Action: duckduck\n",
      "Action Input: current price of macbook pro\u001b[0m\u001b[33;1m\u001b[1;3msnippet: Find the cheapest prices and best deals on current MacBook Pros at AppleInsider., title: Best MacBook Pro Deals for March 2025 | Save up to $1,200, link: https://appleinsider.com/deals/best-macbook-pro-deals, snippet: With power, performance, and premium style, the MacBook Pro represents the pinnacle of Apple's laptop offerings. This comprehensive guide breaks down current MacBook Pro pricing across available sizes, processors, storage capacities, and more. You'll learn how choices at checkout impact final cost, find savings opportunities, and gain insight into Apple's pricing philosophy on its ..., title: How Much Does a MacBook Pro Cost? - The Pricer, link: https://www.thepricer.org/how-much-does-a-macbook-pro-cost/, snippet: Save money on a Mac laptop--including the M4 MacBook Pro--with our round up of MacBook Pro money off deals and offers in the U.S. & U.K., title: Best MacBook Pro Deals: March 2025 | Macworld, link: https://www.macworld.com/article/672811/best-macbook-pro-deals.html, snippet: These are currently the lowest prices available for new M4-powered 14″ MacBook Pros among the Apple retailers we track. For the latest sales and prices, keep an eye on our 14-inch MacBook Pro Price Tracker, updated daily., title: 14-inch M4 MacBook Pros on sale for up to $220 off Apple's MSRP, link: https://www.macprices.net/2025/03/14/14-inch-m4-macbook-pros-on-sale-for-up-to-220-off-apples-msrp/\u001b[0m\u001b[32;1m\u001b[1;3mThought: The observation from the duckduck search doesn't give a single current price. It provides links to sites where I can find the prices. I need to find a specific model and price.  Let's assume a base model 14-inch M4 MacBook Pro.  I'll need to find a price in USD.\n",
      "Action: duckduck\n",
      "Action Input: price of 14 inch M4 macbook pro\u001b[0m\u001b[33;1m\u001b[1;3msnippet: B&H offers free 1-2 day shipping to most US addresses: - 14″ M4 MacBook Pro (16GB/512GB/Space Gray): $1399, $200 off MSRP Their price is the lowest available for this model from any of the Apple retailers we track. For the latest sales and prices, keep an eye on our 14-inch MacBook Pro Price Tracker, updated daily., title: 14-inch M4 MacBook Pro on Holiday sale for $1399, $200 off Apple's MSRP, link: https://www.macprices.net/2024/12/19/14-inch-m4-macbook-pro-on-holiday-sale-for-1399-200-off-apples-msrp/, snippet: Best 2024 14-inch MacBook Pro M4 Pro and M4 Max prices, with exclusive coupon discounts and easy price comparison., title: MacBook Pro 14-inch M4 Pro, M4 Max Best Sale Price Deals, link: https://prices.appleinsider.com/macbook-pro-14-inch-m4-pro, snippet: Here are the lowest prices on 14-inch MacBook Pros with M4 CPUs for Black Friday this year. Save $200 at the following retailers, with prices starting at $1399., title: Here are the lowest prices on 14-inch M4 MacBook Pros for Black Friday ..., link: https://www.macprices.net/2024/11/29/here-are-the-lowest-prices-on-14-inch-m4-macbook-pros-for-black-friday-this-year-take-200-off-msrp/, snippet: - 14″ M4 MacBook Pro (16GB/1TB/Gray): $1599, $200 off MSRP - 14″ M4 MacBook Pro (24GB/1TB/Gray): $1799, $200 off MSRP These are currently the lowest prices available for new M4-powered 14″ MacBook Pros among the Apple retailers we track. For the latest sales and prices, keep an eye on our 14-inch MacBook Pro Price Tracker, updated daily., title: 14-inch M4 MacBook Pros on sale today for $150-$200 off MSRP, link: https://www.macprices.net/2025/01/14/14-inch-m4-macbook-pros-on-sale-today-for-150-200-off-msrp/\u001b[0m\u001b[32;1m\u001b[1;3mThought: I found a price of $1399 USD for a 14-inch M4 MacBook Pro (16GB/512GB).  Now I need to convert that to EUR using the exchange rate.\n",
      "Action: Calculator\n",
      "Action Input: 1399 * 0.85\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 1189.1499999999999\u001b[0m\u001b[32;1m\u001b[1;3mThought: I now know the final answer.\n",
      "Final Answer: A 14-inch M4 MacBook Pro (16GB/512GB) costs $1399 USD.  At an exchange rate of 0.85 EUR per 1 USD, it would cost approximately 1189.15 EUR.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?',\n",
       " 'output': 'A 14-inch M4 MacBook Pro (16GB/512GB) costs $1399 USD.  At an exchange rate of 0.85 EUR per 1 USD, it would cost approximately 1189.15 EUR.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the Price of a MacBook Pro?\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to find the current price of a MacBook Pro in USD and then convert that price to KRW using the given exchange rate.\n",
      "Action: duckduck\n",
      "Action Input: \"current price of a MacBook Pro\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: Apple resellers are hosting a variety of MacBook Pro sales that discount current M4, M4 Pro and M4 Max 14-inch and 16-inch models, in addition to blowout bargains on M3 models. Apple offers two ..., title: Best MacBook Pro Deals for March 2025 | Save up to $1,200 - AppleInsider, link: https://appleinsider.com/deals/best-macbook-pro-deals, snippet: A MacBook Pro cost ranges from just over $1000 to well over $3000. Determining the true cost for the MacBook Pro you need requires close examination of the model options, hardware configurations, and Apple's pricing strategies. Let's examine how Apple prices each MacBook Pro base model and default hardware configuration: MacBook Pro 13-inch, title: How Much Does a MacBook Pro Cost? - The Pricer, link: https://www.thepricer.org/how-much-does-a-macbook-pro-cost/, snippet: Apple announced new 14-inch and 16-inch MacBook Pro models with M4, M4 Pro and M4 Max chips at an event in October 2024. The upgraded MacBook Pro models arrived a year after Apple launched the M3 ..., title: Best MacBook Pro Deals: March 2025 - Macworld, link: https://www.macworld.com/article/672811/best-macbook-pro-deals.html, snippet: The $300 price cut at Best Buy on the standard M4 Pro MacBook Pro 14-inch knocks the price down to a new all-time low of $1,699. Equipped with a 12-core CPU, 16-core GPU, 24GB unified memory and ..., title: Apple MacBook Pro 14-inch M4 Pro Drops to Best $1,699 Price - AppleInsider, link: https://appleinsider.com/articles/24/12/24/apples-14-inch-macbook-pro-with-m4-pro-chip-plunges-to-record-low-1699-today-only\u001b[0m\u001b[32;1m\u001b[1;3mI have found that the price of a MacBook Pro can range from just over $1000 to well over $3000. One specific example is a 14-inch M4 Pro MacBook Pro which is available for $1,699. I will use this price for the calculation. Now I need to convert this price to KRW.\n",
      "Action: Calculator\n",
      "Action Input: 1699 * 1467.60\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 2493452.4\u001b[0m\u001b[32;1m\u001b[1;3mI have the USD price and the exchange rate. I have calculated the price in KRW.\n",
      "Final Answer: The price of the MacBook Pro is $1699, which is equivalent to 2,493,452.4 KRW at the given exchange rate.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in KRW if the exchange rate is 1467.60 KRW for 1 USD?',\n",
       " 'output': 'The price of the MacBook Pro is $1699, which is equivalent to 2,493,452.4 KRW at the given exchange rate.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "search = DuckDuckGoSearchResults()\n",
    "search_tool = Tool(\n",
    "    name=\"duckduck\",\n",
    "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
    "    func=search.run,\n",
    ")\n",
    "\n",
    "# Prepare tools\n",
    "tools = load_tools([\"llm-math\"], llm=gemini)\n",
    "tools.append(search_tool)\n",
    "\n",
    "# Construct the ReAct agent\n",
    "agent = create_react_agent(gemini, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in KRW if the exchange rate is 1467.60 KRW for 1 USD?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
