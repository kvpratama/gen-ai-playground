{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Representation Models for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuning a Pretrained BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since rotten_tomatoes couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\IdeaPad\\.cache\\huggingface\\datasets\\rotten_tomatoes\\default\\0.0.0\\aa13bc287fa6fcab6daf52f0dfb9994269ffea28 (last modified on Sat Mar 29 12:38:20 2025).\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Prepare data and splits\n",
    "tomatoes = load_dataset(\"rotten_tomatoes\")\n",
    "train_data, test_data = tomatoes[\"train\"], tomatoes[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "model_id = \"bert-base-cased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 8530/8530 [00:00<00:00, 10793.48 examples/s]\n",
      "Map: 100%|██████████| 1066/1066 [00:00<00:00, 7661.35 examples/s] \n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Pad to the longest sequence in the batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "   \"\"\"Tokenize input data\"\"\"\n",
    "   return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# Tokenize train/test data\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_data.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculate F1 score\"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    load_f1 = evaluate.load(\"f1\")\n",
    "    f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "    return {\"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Training arguments for parameter tuning\n",
    "training_args = TrainingArguments(\n",
    "   \"model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=1,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer which executes the training process\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▎| 500/534 [02:47<00:13,  2.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4205, 'grad_norm': 15.350017547607422, 'learning_rate': 1.2734082397003748e-06, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534/534 [03:07<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 187.2086, 'train_samples_per_second': 45.564, 'train_steps_per_second': 2.852, 'train_loss': 0.41636749867642864, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=534, training_loss=0.41636749867642864, metrics={'train_runtime': 187.2086, 'train_samples_per_second': 45.564, 'train_steps_per_second': 2.852, 'total_flos': 227605451772240.0, 'train_loss': 0.41636749867642864, 'epoch': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 6.79k/6.79k [00:00<00:00, 4.52MB/s]\n",
      "100%|██████████| 67/67 [00:08<00:00,  7.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.37474557757377625,\n",
       " 'eval_f1': 0.8462255358807083,\n",
       " 'eval_runtime': 8.9091,\n",
       " 'eval_samples_per_second': 119.653,\n",
       " 'eval_steps_per_second': 7.52,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "model_id = \"bert-base-cased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert.embeddings.word_embeddings.weight\n",
      "bert.embeddings.position_embeddings.weight\n",
      "bert.embeddings.token_type_embeddings.weight\n",
      "bert.embeddings.LayerNorm.weight\n",
      "bert.embeddings.LayerNorm.bias\n",
      "bert.encoder.layer.0.attention.self.query.weight\n",
      "bert.encoder.layer.0.attention.self.query.bias\n",
      "bert.encoder.layer.0.attention.self.key.weight\n",
      "bert.encoder.layer.0.attention.self.key.bias\n",
      "bert.encoder.layer.0.attention.self.value.weight\n",
      "bert.encoder.layer.0.attention.self.value.bias\n",
      "bert.encoder.layer.0.attention.output.dense.weight\n",
      "bert.encoder.layer.0.attention.output.dense.bias\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.0.intermediate.dense.weight\n",
      "bert.encoder.layer.0.intermediate.dense.bias\n",
      "bert.encoder.layer.0.output.dense.weight\n",
      "bert.encoder.layer.0.output.dense.bias\n",
      "bert.encoder.layer.0.output.LayerNorm.weight\n",
      "bert.encoder.layer.0.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.attention.self.query.weight\n",
      "bert.encoder.layer.1.attention.self.query.bias\n",
      "bert.encoder.layer.1.attention.self.key.weight\n",
      "bert.encoder.layer.1.attention.self.key.bias\n",
      "bert.encoder.layer.1.attention.self.value.weight\n",
      "bert.encoder.layer.1.attention.self.value.bias\n",
      "bert.encoder.layer.1.attention.output.dense.weight\n",
      "bert.encoder.layer.1.attention.output.dense.bias\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.1.intermediate.dense.weight\n",
      "bert.encoder.layer.1.intermediate.dense.bias\n",
      "bert.encoder.layer.1.output.dense.weight\n",
      "bert.encoder.layer.1.output.dense.bias\n",
      "bert.encoder.layer.1.output.LayerNorm.weight\n",
      "bert.encoder.layer.1.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.attention.self.query.weight\n",
      "bert.encoder.layer.2.attention.self.query.bias\n",
      "bert.encoder.layer.2.attention.self.key.weight\n",
      "bert.encoder.layer.2.attention.self.key.bias\n",
      "bert.encoder.layer.2.attention.self.value.weight\n",
      "bert.encoder.layer.2.attention.self.value.bias\n",
      "bert.encoder.layer.2.attention.output.dense.weight\n",
      "bert.encoder.layer.2.attention.output.dense.bias\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.2.intermediate.dense.weight\n",
      "bert.encoder.layer.2.intermediate.dense.bias\n",
      "bert.encoder.layer.2.output.dense.weight\n",
      "bert.encoder.layer.2.output.dense.bias\n",
      "bert.encoder.layer.2.output.LayerNorm.weight\n",
      "bert.encoder.layer.2.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.attention.self.query.weight\n",
      "bert.encoder.layer.3.attention.self.query.bias\n",
      "bert.encoder.layer.3.attention.self.key.weight\n",
      "bert.encoder.layer.3.attention.self.key.bias\n",
      "bert.encoder.layer.3.attention.self.value.weight\n",
      "bert.encoder.layer.3.attention.self.value.bias\n",
      "bert.encoder.layer.3.attention.output.dense.weight\n",
      "bert.encoder.layer.3.attention.output.dense.bias\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.3.intermediate.dense.weight\n",
      "bert.encoder.layer.3.intermediate.dense.bias\n",
      "bert.encoder.layer.3.output.dense.weight\n",
      "bert.encoder.layer.3.output.dense.bias\n",
      "bert.encoder.layer.3.output.LayerNorm.weight\n",
      "bert.encoder.layer.3.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.attention.self.query.weight\n",
      "bert.encoder.layer.4.attention.self.query.bias\n",
      "bert.encoder.layer.4.attention.self.key.weight\n",
      "bert.encoder.layer.4.attention.self.key.bias\n",
      "bert.encoder.layer.4.attention.self.value.weight\n",
      "bert.encoder.layer.4.attention.self.value.bias\n",
      "bert.encoder.layer.4.attention.output.dense.weight\n",
      "bert.encoder.layer.4.attention.output.dense.bias\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.4.intermediate.dense.weight\n",
      "bert.encoder.layer.4.intermediate.dense.bias\n",
      "bert.encoder.layer.4.output.dense.weight\n",
      "bert.encoder.layer.4.output.dense.bias\n",
      "bert.encoder.layer.4.output.LayerNorm.weight\n",
      "bert.encoder.layer.4.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.attention.self.query.weight\n",
      "bert.encoder.layer.5.attention.self.query.bias\n",
      "bert.encoder.layer.5.attention.self.key.weight\n",
      "bert.encoder.layer.5.attention.self.key.bias\n",
      "bert.encoder.layer.5.attention.self.value.weight\n",
      "bert.encoder.layer.5.attention.self.value.bias\n",
      "bert.encoder.layer.5.attention.output.dense.weight\n",
      "bert.encoder.layer.5.attention.output.dense.bias\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.5.intermediate.dense.weight\n",
      "bert.encoder.layer.5.intermediate.dense.bias\n",
      "bert.encoder.layer.5.output.dense.weight\n",
      "bert.encoder.layer.5.output.dense.bias\n",
      "bert.encoder.layer.5.output.LayerNorm.weight\n",
      "bert.encoder.layer.5.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.attention.self.query.weight\n",
      "bert.encoder.layer.6.attention.self.query.bias\n",
      "bert.encoder.layer.6.attention.self.key.weight\n",
      "bert.encoder.layer.6.attention.self.key.bias\n",
      "bert.encoder.layer.6.attention.self.value.weight\n",
      "bert.encoder.layer.6.attention.self.value.bias\n",
      "bert.encoder.layer.6.attention.output.dense.weight\n",
      "bert.encoder.layer.6.attention.output.dense.bias\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.6.intermediate.dense.weight\n",
      "bert.encoder.layer.6.intermediate.dense.bias\n",
      "bert.encoder.layer.6.output.dense.weight\n",
      "bert.encoder.layer.6.output.dense.bias\n",
      "bert.encoder.layer.6.output.LayerNorm.weight\n",
      "bert.encoder.layer.6.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.attention.self.query.weight\n",
      "bert.encoder.layer.7.attention.self.query.bias\n",
      "bert.encoder.layer.7.attention.self.key.weight\n",
      "bert.encoder.layer.7.attention.self.key.bias\n",
      "bert.encoder.layer.7.attention.self.value.weight\n",
      "bert.encoder.layer.7.attention.self.value.bias\n",
      "bert.encoder.layer.7.attention.output.dense.weight\n",
      "bert.encoder.layer.7.attention.output.dense.bias\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.7.intermediate.dense.weight\n",
      "bert.encoder.layer.7.intermediate.dense.bias\n",
      "bert.encoder.layer.7.output.dense.weight\n",
      "bert.encoder.layer.7.output.dense.bias\n",
      "bert.encoder.layer.7.output.LayerNorm.weight\n",
      "bert.encoder.layer.7.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.attention.self.query.weight\n",
      "bert.encoder.layer.8.attention.self.query.bias\n",
      "bert.encoder.layer.8.attention.self.key.weight\n",
      "bert.encoder.layer.8.attention.self.key.bias\n",
      "bert.encoder.layer.8.attention.self.value.weight\n",
      "bert.encoder.layer.8.attention.self.value.bias\n",
      "bert.encoder.layer.8.attention.output.dense.weight\n",
      "bert.encoder.layer.8.attention.output.dense.bias\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.8.intermediate.dense.weight\n",
      "bert.encoder.layer.8.intermediate.dense.bias\n",
      "bert.encoder.layer.8.output.dense.weight\n",
      "bert.encoder.layer.8.output.dense.bias\n",
      "bert.encoder.layer.8.output.LayerNorm.weight\n",
      "bert.encoder.layer.8.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.attention.self.query.weight\n",
      "bert.encoder.layer.9.attention.self.query.bias\n",
      "bert.encoder.layer.9.attention.self.key.weight\n",
      "bert.encoder.layer.9.attention.self.key.bias\n",
      "bert.encoder.layer.9.attention.self.value.weight\n",
      "bert.encoder.layer.9.attention.self.value.bias\n",
      "bert.encoder.layer.9.attention.output.dense.weight\n",
      "bert.encoder.layer.9.attention.output.dense.bias\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.9.intermediate.dense.weight\n",
      "bert.encoder.layer.9.intermediate.dense.bias\n",
      "bert.encoder.layer.9.output.dense.weight\n",
      "bert.encoder.layer.9.output.dense.bias\n",
      "bert.encoder.layer.9.output.LayerNorm.weight\n",
      "bert.encoder.layer.9.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.attention.self.query.weight\n",
      "bert.encoder.layer.10.attention.self.query.bias\n",
      "bert.encoder.layer.10.attention.self.key.weight\n",
      "bert.encoder.layer.10.attention.self.key.bias\n",
      "bert.encoder.layer.10.attention.self.value.weight\n",
      "bert.encoder.layer.10.attention.self.value.bias\n",
      "bert.encoder.layer.10.attention.output.dense.weight\n",
      "bert.encoder.layer.10.attention.output.dense.bias\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.10.intermediate.dense.weight\n",
      "bert.encoder.layer.10.intermediate.dense.bias\n",
      "bert.encoder.layer.10.output.dense.weight\n",
      "bert.encoder.layer.10.output.dense.bias\n",
      "bert.encoder.layer.10.output.LayerNorm.weight\n",
      "bert.encoder.layer.10.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.attention.self.query.weight\n",
      "bert.encoder.layer.11.attention.self.query.bias\n",
      "bert.encoder.layer.11.attention.self.key.weight\n",
      "bert.encoder.layer.11.attention.self.key.bias\n",
      "bert.encoder.layer.11.attention.self.value.weight\n",
      "bert.encoder.layer.11.attention.self.value.bias\n",
      "bert.encoder.layer.11.attention.output.dense.weight\n",
      "bert.encoder.layer.11.attention.output.dense.bias\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "bert.encoder.layer.11.intermediate.dense.weight\n",
      "bert.encoder.layer.11.intermediate.dense.bias\n",
      "bert.encoder.layer.11.output.dense.weight\n",
      "bert.encoder.layer.11.output.dense.bias\n",
      "bert.encoder.layer.11.output.LayerNorm.weight\n",
      "bert.encoder.layer.11.output.LayerNorm.bias\n",
      "bert.pooler.dense.weight\n",
      "bert.pooler.dense.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "# Print layer names\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "\n",
    "     # Trainable classification head\n",
    "     if name.startswith(\"classifier\"):\n",
    "        param.requires_grad = True\n",
    "\n",
    "      # Freeze everything else\n",
    "     else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: bert.embeddings.word_embeddings.weight ----- False\n",
      "Parameter: bert.embeddings.position_embeddings.weight ----- False\n",
      "Parameter: bert.embeddings.token_type_embeddings.weight ----- False\n",
      "Parameter: bert.embeddings.LayerNorm.weight ----- False\n",
      "Parameter: bert.embeddings.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.0.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.0.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.1.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.1.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.2.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.2.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.3.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.3.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.4.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.4.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.5.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.5.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.6.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.6.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.7.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.7.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.8.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.8.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.9.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.9.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.10.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.10.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.query.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.query.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.key.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.key.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.value.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.self.value.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.attention.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.intermediate.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.intermediate.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.output.dense.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.output.dense.bias ----- False\n",
      "Parameter: bert.encoder.layer.11.output.LayerNorm.weight ----- False\n",
      "Parameter: bert.encoder.layer.11.output.LayerNorm.bias ----- False\n",
      "Parameter: bert.pooler.dense.weight ----- False\n",
      "Parameter: bert.pooler.dense.bias ----- False\n",
      "Parameter: classifier.weight ----- True\n",
      "Parameter: classifier.bias ----- True\n"
     ]
    }
   ],
   "source": [
    "# We can check whether the model was correctly updated\n",
    "for name, param in model.named_parameters():\n",
    "     print(f\"Parameter: {name} ----- {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 501/534 [01:13<00:05,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6933, 'grad_norm': 4.200374603271484, 'learning_rate': 1.2734082397003748e-06, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534/534 [01:21<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 81.5224, 'train_samples_per_second': 104.634, 'train_steps_per_second': 6.55, 'train_loss': 0.693643527084522, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=534, training_loss=0.693643527084522, metrics={'train_runtime': 81.5224, 'train_samples_per_second': 104.634, 'train_steps_per_second': 6.55, 'total_flos': 227605451772240.0, 'train_loss': 0.693643527084522, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Trainer which executes the training process\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:12<00:00,  5.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6833594441413879,\n",
       " 'eval_f1': 0.6201550387596899,\n",
       " 'eval_runtime': 12.3319,\n",
       " 'eval_samples_per_second': 86.442,\n",
       " 'eval_steps_per_second': 5.433,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freeze everything up until encoder block 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      " 94%|█████████▎| 500/534 [01:38<00:07,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4744, 'grad_norm': 4.441214084625244, 'learning_rate': 1.2734082397003748e-06, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534/534 [01:49<00:00,  4.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 109.09, 'train_samples_per_second': 78.192, 'train_steps_per_second': 4.895, 'train_loss': 0.46974131587739293, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:12<00:00,  5.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.4083769917488098,\n",
       " 'eval_f1': 0.8128031037827352,\n",
       " 'eval_runtime': 12.3918,\n",
       " 'eval_samples_per_second': 86.025,\n",
       " 'eval_steps_per_second': 5.407,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model_id = \"bert-base-cased\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Encoder block 10 starts at index 165 and\n",
    "# we freeze everything before that block\n",
    "for index, (name, param) in enumerate(model.named_parameters()):\n",
    "    if index < 165:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Trainer which executes the training process\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# for index in range(12):\n",
    "#     # Re-load model\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "#     # Freeze encoder blocks 0-index\n",
    "#     for name, param in model.named_parameters():\n",
    "#         if \"layer\" in name:\n",
    "#             layer_nr = int(name.split(\"layer\")[1].split(\".\")[1])\n",
    "#             if layer_nr <= index:\n",
    "#                 param.requires_grad = False\n",
    "#         else:\n",
    "#             param.requires_grad = True\n",
    "\n",
    "#     # Train\n",
    "#     trainer = Trainer(\n",
    "#       model=model,\n",
    "#       args=training_args,\n",
    "#       train_dataset=tokenized_train,\n",
    "#       eval_dataset=tokenized_test,\n",
    "#       tokenizer=tokenizer,\n",
    "#       data_collator=data_collator,\n",
    "#       compute_metrics=compute_metrics,\n",
    "#     )\n",
    "#     trainer.train()\n",
    "\n",
    "#     # Evaluate\n",
    "#     score = trainer.evaluate()[\"eval_f1\"]\n",
    "#     scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create Figure\n",
    "plt.figure(figsize=(8,4))\n",
    "\n",
    "# Prepare Data\n",
    "x = [f\"0-{index}\" for index in range(12)]\n",
    "x[0] = \"None\"\n",
    "x[-1] = \"All\"\n",
    "y = [\n",
    "    0.8541862652869239,\n",
    "    0.8525519848771267,\n",
    "    0.8514664143803217,\n",
    "    0.8506616257088847,\n",
    "    0.8398104265402844,\n",
    "    0.8391345249294448,\n",
    "    0.8377358490566037,\n",
    "    0.8433962264150944,\n",
    "    0.8258801141769743,\n",
    "    0.816247582205029,\n",
    "    0.7917485265225934,\n",
    "    0.7019400352733686\n",
    "][::-1]\n",
    "\n",
    "# Stylize Figure\n",
    "plt.grid(color='#ECEFF1')\n",
    "plt.axvline(x=4, color=\"#EC407A\", linestyle=\"--\")\n",
    "plt.title(\"Effect of Frozen Encoder Blocks on Training Performance\")\n",
    "plt.ylabel(\"F1-score\")\n",
    "plt.xlabel(\"Trainable encoder blocks\")\n",
    "\n",
    "# Plot Data\n",
    "plt.plot(x, y, color=\"black\")\n",
    "\n",
    "# Additional Annotation\n",
    "plt.annotate(\n",
    "    'Performance stabilizing',\n",
    "    xy=(4, y[4]),\n",
    "    xytext=(4.5, y[4]-.05),\n",
    "    arrowprops=dict(\n",
    "        arrowstyle=\"-|>\",\n",
    "        connectionstyle=\"arc3\",\n",
    "        color=\"#00ACC1\")\n",
    ")\n",
    "plt.savefig(\"multiple_frozen_blocks.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from setfit import sample_dataset\n",
    "\n",
    "# We simulate a few-shot setting by sampling 16 examples per class\n",
    "sampled_train_data = sample_dataset(tomatoes[\"train\"], num_samples=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from setfit import SetFitModel\n",
    "\n",
    "# Load a pre-trained SentenceTransformer model\n",
    "model = SetFitModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\datasets\\utils\\_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n",
      "Map: 100%|██████████| 32/32 [00:00<00:00, 4225.20 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from setfit import TrainingArguments as SetFitTrainingArguments\n",
    "from setfit import Trainer as SetFitTrainer\n",
    "\n",
    "# Define training arguments\n",
    "args = SetFitTrainingArguments(\n",
    "    num_epochs=3, # The number of epochs to use for contrastive learning\n",
    "    num_iterations=20  # The number of text pairs to generate\n",
    ")\n",
    "args.eval_strategy = args.evaluation_strategy\n",
    "\n",
    "# Create trainer\n",
    "trainer = SetFitTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=sampled_train_data,\n",
    "    eval_dataset=test_data,\n",
    "    metric=\"f1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 1280\n",
      "  Batch size = 16\n",
      "  Num epochs = 3\n",
      "  0%|          | 1/240 [00:04<18:26,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.3085, 'grad_norm': 2.036525011062622, 'learning_rate': 8.333333333333333e-07, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 50/240 [00:41<02:19,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.1157, 'grad_norm': 0.15513768792152405, 'learning_rate': 1.7592592592592595e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 100/240 [01:25<02:27,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.001, 'grad_norm': 0.05414820834994316, 'learning_rate': 1.2962962962962964e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 150/240 [02:12<01:23,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0005, 'grad_norm': 0.01988757960498333, 'learning_rate': 8.333333333333334e-06, 'epoch': 1.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 200/240 [02:49<00:27,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_loss': 0.0003, 'grad_norm': 0.012995349243283272, 'learning_rate': 3.7037037037037037e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 240/240 [03:25<00:00,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 205.4188, 'train_samples_per_second': 18.694, 'train_steps_per_second': 1.168, 'train_loss': 0.02534471363760531, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'f1': 0.8464491362763915}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on our test data\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: #000;\n",
       "  --sklearn-color-text-muted: #666;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: flex;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "  align-items: start;\n",
       "  justify-content: space-between;\n",
       "  gap: 0.5em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label .caption {\n",
       "  font-size: 0.6rem;\n",
       "  font-weight: lighter;\n",
       "  color: var(--sklearn-color-text-muted);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 0.5em;\n",
       "  text-align: center;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>LogisticRegression</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model_head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continue Pretraining with Masked Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"1115\"](img/11.15.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Load model for Masked Language Modeling (MLM)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\datasets\\utils\\_dill.py:385: DeprecationWarning: co_lnotab is deprecated, use co_lines instead.\n",
      "  obj.co_lnotab,  # for < python 3.10 [not counted in args]\n",
      "Map: 100%|██████████| 8530/8530 [00:00<00:00, 26099.58 examples/s]\n",
      "Map: 100%|██████████| 1066/1066 [00:00<00:00, 13920.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "   return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# Tokenize data\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_train = tokenized_train.remove_columns(\"label\")\n",
    "tokenized_test = test_data.map(preprocess_function, batched=True)\n",
    "tokenized_test = tokenized_test.remove_columns(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# Masking Tokens\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Training arguments for parameter tuning\n",
    "training_args = TrainingArguments(\n",
    "   \"model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=1,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   report_to=\"none\"\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "  1%|          | 31/5340 [05:26<46:13,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5431, 'grad_norm': 13.505681037902832, 'learning_rate': 1.2734082397003748e-06, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      "100%|██████████| 534/534 [05:11<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 311.5539, 'train_samples_per_second': 27.379, 'train_steps_per_second': 1.714, 'train_loss': 2.5445126594229137, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Save pre-trained tokenizer\n",
    "tokenizer.save_pretrained(\"mlm\")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save updated model\n",
    "model.save_pretrained(\"mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\io\\image.py:14: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> What a horrible idea!\n",
      ">>> What a horrible dream!\n",
      ">>> What a horrible thing!\n",
      ">>> What a horrible day!\n",
      ">>> What a horrible thought!\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load and create predictions\n",
    "mask_filler = pipeline(\"fill-mask\", model=\"bert-base-cased\")\n",
    "preds = mask_filler(\"What a horrible [MASK]!\")\n",
    "\n",
    "# Print results\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> What a horrible movie!\n",
      ">>> What a horrible story!\n",
      ">>> What a horrible mess!\n",
      ">>> What a horrible thing!\n",
      ">>> What a horrible joke!\n"
     ]
    }
   ],
   "source": [
    "# Load and create predictions\n",
    "mask_filler = pipeline(\"fill-mask\", model=\"mlm\")\n",
    "preds = mask_filler(\"What a horrible [MASK]!\")\n",
    "\n",
    "# Print results\n",
    "for pred in preds:\n",
    "    print(f\">>> {pred['sequence']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at mlm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 8530/8530 [00:01<00:00, 7863.13 examples/s]\n",
      "Map: 100%|██████████| 1066/1066 [00:00<00:00, 10331.42 examples/s]\n",
      " 94%|█████████▎| 500/534 [02:56<00:10,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4144, 'grad_norm': 13.293235778808594, 'learning_rate': 1.2734082397003748e-06, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534/534 [03:15<00:00,  2.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 195.2002, 'train_samples_per_second': 43.699, 'train_steps_per_second': 2.736, 'train_loss': 0.4097407212418117, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=534, training_loss=0.4097407212418117, metrics={'train_runtime': 195.2002, 'train_samples_per_second': 43.699, 'train_steps_per_second': 2.736, 'total_flos': 227605451772240.0, 'train_loss': 0.4097407212418117, 'epoch': 1.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "model_id = \"mlm\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "# Pad to the longest sequence in the batch\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "   \"\"\"Tokenize input data\"\"\"\n",
    "   return tokenizer(examples[\"text\"], truncation=True)\n",
    "\n",
    "# Tokenize train/test data\n",
    "tokenized_train = train_data.map(preprocess_function, batched=True)\n",
    "tokenized_test = test_data.map(preprocess_function, batched=True)\n",
    "\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# Training arguments for parameter tuning\n",
    "training_args = TrainingArguments(\n",
    "   \"model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=1,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Trainer which executes the training process\n",
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [00:09<00:00,  6.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.3713599145412445,\n",
       " 'eval_f1': 0.8563327032136105,\n",
       " 'eval_runtime': 10.2177,\n",
       " 'eval_samples_per_second': 104.328,\n",
       " 'eval_steps_per_second': 6.557,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named-entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![\"1119\"](img/11.19.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\IdeaPad\\.cache\\huggingface\\hub\\datasets--conll2003. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading data: 100%|██████████| 983k/983k [00:00<00:00, 3.89MB/s]\n",
      "Generating train split: 100%|██████████| 14041/14041 [00:02<00:00, 6142.50 examples/s]\n",
      "Generating validation split: 100%|██████████| 3250/3250 [00:00<00:00, 4934.95 examples/s]\n",
      "Generating test split: 100%|██████████| 3453/3453 [00:00<00:00, 6189.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# The CoNLL-2003 dataset for NER\n",
    "dataset = load_dataset(\"conll2003\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '848',\n",
       " 'tokens': ['Dean',\n",
       "  'Palmer',\n",
       "  'hit',\n",
       "  'his',\n",
       "  '30th',\n",
       "  'homer',\n",
       "  'for',\n",
       "  'the',\n",
       "  'Rangers',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 22, 38, 29, 16, 21, 15, 12, 23, 7],\n",
       " 'chunk_tags': [11, 12, 21, 11, 12, 12, 13, 11, 12, 0],\n",
       " 'ner_tags': [1, 2, 0, 0, 0, 0, 0, 0, 3, 0]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = dataset[\"train\"][848]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-PER': 1,\n",
       " 'I-PER': 2,\n",
       " 'B-ORG': 3,\n",
       " 'I-ORG': 4,\n",
       " 'B-LOC': 5,\n",
       " 'I-LOC': 6,\n",
       " 'B-MISC': 7,\n",
       " 'I-MISC': 8}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id = {\n",
    "    'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4,\n",
    "    'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8\n",
    "}\n",
    "id2label = {index: label for label, index in label2id.items()}\n",
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"bert-base-cased\",\n",
    "    num_labels=len(id2label),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'Dean',\n",
       " 'Palmer',\n",
       " 'hit',\n",
       " 'his',\n",
       " '30th',\n",
       " 'home',\n",
       " '##r',\n",
       " 'for',\n",
       " 'the',\n",
       " 'Rangers',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split individual tokens into sub-tokens\n",
    "token_ids = tokenizer(example[\"tokens\"], is_split_into_words=True)[\"input_ids\"]\n",
    "sub_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "sub_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/14041 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 14041/14041 [00:01<00:00, 9232.40 examples/s] \n",
      "Map: 100%|██████████| 3250/3250 [00:00<00:00, 13100.50 examples/s]\n",
      "Map: 100%|██████████| 3453/3453 [00:00<00:00, 12756.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def align_labels(examples):\n",
    "    token_ids = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = examples[\"ner_tags\"]\n",
    "\n",
    "    updated_labels = []\n",
    "    for index, label in enumerate(labels):\n",
    "\n",
    "        # Map tokens to their respective word\n",
    "        word_ids = token_ids.word_ids(batch_index=index)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "\n",
    "            # The start of a new word\n",
    "            if word_idx != previous_word_idx:\n",
    "\n",
    "                previous_word_idx = word_idx\n",
    "                updated_label = -100 if word_idx is None else label[word_idx]\n",
    "                label_ids.append(updated_label)\n",
    "\n",
    "            # Special token is -100\n",
    "            elif word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            else:\n",
    "                updated_label = label[word_idx]\n",
    "                if updated_label % 2 == 1:\n",
    "                    updated_label += 1\n",
    "                label_ids.append(updated_label)\n",
    "\n",
    "        updated_labels.append(label_ids)\n",
    "\n",
    "    token_ids[\"labels\"] = updated_labels\n",
    "    return token_ids\n",
    "\n",
    "tokenized = dataset.map(align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [1, 2, 0, 0, 0, 0, 0, 0, 3, 0]\n",
      "Updated: [-100, 1, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, -100]\n"
     ]
    }
   ],
   "source": [
    "# Difference between original and updated labels\n",
    "print(f\"Original: {example['ner_tags']}\")\n",
    "print(f\"Updated: {tokenized['train'][848]['labels']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "# Load sequential evaluation\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    # Create predictions\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=2)\n",
    "\n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "\n",
    "    # Document-level iteration\n",
    "    for prediction, label in zip(predictions, labels):\n",
    "\n",
    "      # token-level iteration\n",
    "      for token_prediction, token_label in zip(prediction, label):\n",
    "\n",
    "        # We ignore special tokens\n",
    "        if token_label != -100:\n",
    "          true_predictions.append([id2label[token_prediction]])\n",
    "          true_labels.append([id2label[token_label]])\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\"f1\": results[\"overall_f1\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# Token-classification Data Collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 500/878 [03:30<03:25,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2276, 'grad_norm': 1.3758715391159058, 'learning_rate': 8.610478359908885e-06, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 878/878 [06:36<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 396.0803, 'train_samples_per_second': 35.45, 'train_steps_per_second': 2.217, 'train_loss': 0.16488116966021632, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=878, training_loss=0.16488116966021632, metrics={'train_runtime': 396.0803, 'train_samples_per_second': 35.45, 'train_steps_per_second': 2.217, 'total_flos': 351240792638148.0, 'train_loss': 0.16488116966021632, 'epoch': 1.0})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training arguments for parameter tuning\n",
    "training_args = TrainingArguments(\n",
    "   \"model\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=16,\n",
    "   per_device_eval_batch_size=16,\n",
    "   num_train_epochs=1,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   report_to=\"none\"\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [00:32<00:00,  6.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.14390742778778076,\n",
       " 'eval_f1': 0.902149033570702,\n",
       " 'eval_runtime': 32.929,\n",
       " 'eval_samples_per_second': 104.862,\n",
       " 'eval_steps_per_second': 6.56,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model on our test data\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-PER',\n",
       "  'score': 0.9917218,\n",
       "  'index': 4,\n",
       "  'word': 'Ma',\n",
       "  'start': 11,\n",
       "  'end': 13},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.97077405,\n",
       "  'index': 5,\n",
       "  'word': '##arte',\n",
       "  'start': 13,\n",
       "  'end': 17},\n",
       " {'entity': 'I-PER',\n",
       "  'score': 0.984021,\n",
       "  'index': 6,\n",
       "  'word': '##n',\n",
       "  'start': 17,\n",
       "  'end': 18}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Save our fine-tuned model\n",
    "trainer.save_model(\"ner_model\")\n",
    "\n",
    "# Run inference on the fine-tuned model\n",
    "token_classifier = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"ner_model\",\n",
    ")\n",
    "token_classifier(\"My name is Maarten.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
