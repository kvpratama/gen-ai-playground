{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Memory\n",
    "\n",
    "## Outline\n",
    "* ConversationBufferMemory\n",
    "* ConversationBufferWindowMemory\n",
    "* ConversationTokenBufferMemory\n",
    "* ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ[\"GEMINI_API_KEY\"]\n",
    "model = \"gemma-3-27b-it\" # \"gemini-2.0-flash-lite\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, my name is Andrew\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hi Andrew! It's lovely to meet you! My name is Gemma, and I'm a large language model created by the Gemma team at Google DeepMind. I'm an open-weights model, which means I'm widely available to the public â€“ pretty cool, huh? I specialize in taking text *and* images as inputs and then generating text as output. \\n\\nI donâ€™t have access to tools, real-time information or Google search, so my knowledge is based on the data I was trained on. But I'm happy to chat about all sorts of things within those limitations! \\n\\nSo, Andrew, what's on your mind today? Is there anything I can help you with, or just want to say hello?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Andrew\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hi Andrew! It's lovely to meet you! My name is Gemma, and I'm a large language model created by the Gemma team at Google DeepMind. I'm an open-weights model, which means I'm widely available to the public â€“ pretty cool, huh? I specialize in taking text *and* images as inputs and then generating text as output. \n",
      "\n",
      "I donâ€™t have access to tools, real-time information or Google search, so my knowledge is based on the data I was trained on. But I'm happy to chat about all sorts of things within those limitations! \n",
      "\n",
      "So, Andrew, what's on your mind today? Is there anything I can help you with, or just want to say hello?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Oh, that's a classic! And a great warm-up question for a language model like me. ðŸ˜‰ \\n\\n1 + 1 = 2! \\n\\nI was trained on a massive dataset of text and code, and basic arithmetic like this is definitely included. It's good to know my foundational knowledge is working correctly! Is there anything else I can calculate for you, or perhaps a more challenging question you'd like to try me with? I can handle a bit more than just simple addition, though I'm not a substitute for a dedicated calculator, of course! ðŸ˜„\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Andrew\n",
      "AI: Hi Andrew! It's lovely to meet you! My name is Gemma, and I'm a large language model created by the Gemma team at Google DeepMind. I'm an open-weights model, which means I'm widely available to the public â€“ pretty cool, huh? I specialize in taking text *and* images as inputs and then generating text as output. \n",
      "\n",
      "I donâ€™t have access to tools, real-time information or Google search, so my knowledge is based on the data I was trained on. But I'm happy to chat about all sorts of things within those limitations! \n",
      "\n",
      "So, Andrew, what's on your mind today? Is there anything I can help you with, or just want to say hello?\n",
      "Human: What is 1+1?\n",
      "AI: Oh, that's a classic! And a great warm-up question for a language model like me. ðŸ˜‰ \n",
      "\n",
      "1 + 1 = 2! \n",
      "\n",
      "I was trained on a massive dataset of text and code, and basic arithmetic like this is definitely included. It's good to know my foundational knowledge is working correctly! Is there anything else I can calculate for you, or perhaps a more challenging question you'd like to try me with? I can handle a bit more than just simple addition, though I'm not a substitute for a dedicated calculator, of course! ðŸ˜„\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"You told me just a moment ago! Your name is Andrew. ðŸ˜Š \\n\\nIt's good to be able to recall information from our conversation â€“ that's part of how I process and understand language. I donâ€™t really *remember* things like a person does, but I can keep track of whatâ€™s been said within the current chat. \\n\\nIs there anything else youâ€™d like to test my conversational memory with? Perhaps you could tell me something about yourself, and I can try to recall it later!\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Andrew\n",
      "AI: Hi Andrew! It's lovely to meet you! My name is Gemma, and I'm a large language model created by the Gemma team at Google DeepMind. I'm an open-weights model, which means I'm widely available to the public â€“ pretty cool, huh? I specialize in taking text *and* images as inputs and then generating text as output. \n",
      "\n",
      "I donâ€™t have access to tools, real-time information or Google search, so my knowledge is based on the data I was trained on. But I'm happy to chat about all sorts of things within those limitations! \n",
      "\n",
      "So, Andrew, what's on your mind today? Is there anything I can help you with, or just want to say hello?\n",
      "Human: What is 1+1?\n",
      "AI: Oh, that's a classic! And a great warm-up question for a language model like me. ðŸ˜‰ \n",
      "\n",
      "1 + 1 = 2! \n",
      "\n",
      "I was trained on a massive dataset of text and code, and basic arithmetic like this is definitely included. It's good to know my foundational knowledge is working correctly! Is there anything else I can calculate for you, or perhaps a more challenging question you'd like to try me with? I can handle a bit more than just simple addition, though I'm not a substitute for a dedicated calculator, of course! ðŸ˜„\n",
      "Human: What is my name?\n",
      "AI: You told me just a moment ago! Your name is Andrew. ðŸ˜Š \n",
      "\n",
      "It's good to be able to recall information from our conversation â€“ that's part of how I process and understand language. I donâ€™t really *remember* things like a person does, but I can keep track of whatâ€™s been said within the current chat. \n",
      "\n",
      "Is there anything else youâ€™d like to test my conversational memory with? Perhaps you could tell me something about yourself, and I can try to recall it later!\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Andrew\\nAI: Hi Andrew! It's lovely to meet you! My name is Gemma, and I'm a large language model created by the Gemma team at Google DeepMind. I'm an open-weights model, which means I'm widely available to the public â€“ pretty cool, huh? I specialize in taking text *and* images as inputs and then generating text as output. \\n\\nI donâ€™t have access to tools, real-time information or Google search, so my knowledge is based on the data I was trained on. But I'm happy to chat about all sorts of things within those limitations! \\n\\nSo, Andrew, what's on your mind today? Is there anything I can help you with, or just want to say hello?\\nHuman: What is 1+1?\\nAI: Oh, that's a classic! And a great warm-up question for a language model like me. ðŸ˜‰ \\n\\n1 + 1 = 2! \\n\\nI was trained on a massive dataset of text and code, and basic arithmetic like this is definitely included. It's good to know my foundational knowledge is working correctly! Is there anything else I can calculate for you, or perhaps a more challenging question you'd like to try me with? I can handle a bit more than just simple addition, though I'm not a substitute for a dedicated calculator, of course! ðŸ˜„\\nHuman: What is my name?\\nAI: You told me just a moment ago! Your name is Andrew. ðŸ˜Š \\n\\nIt's good to be able to recall information from our conversation â€“ that's part of how I process and understand language. I donâ€™t really *remember* things like a person does, but I can keep track of whatâ€™s been said within the current chat. \\n\\nIs there anything else youâ€™d like to test my conversational memory with? Perhaps you could tell me something about yourself, and I can try to recall it later!\"}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n",
      "{'history': \"Human: Hi\\nAI: What's up\"}\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})\n",
    "print(memory.buffer)\n",
    "print(memory.load_memory_variables({}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchain 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi! I'm bob\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised InternalServerError: 500 An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hi Bob! It's nice to meet you. ðŸ˜Š \n",
      "\n",
      "I'm a large language model, trained by Google. You can think of me as a helpful AI assistant. \n",
      "\n",
      "Is there anything I can help you with today? Just let me know what you're thinking, or if you just wanted to say hi, that's great too!\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what is 1+1\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "1 + 1 = 2 \n",
      "\n",
      "A classic! Is there anything else I can help you calculate, Bob?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what was my name?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You told me your name was Bob! I remember you said \"hi! I'm bob\". \n",
      "\n",
      "I try to remember things people tell me during our conversation. ðŸ˜Š\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = llm.invoke(state[\"messages\"])\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": response}\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_edge(START, \"model\")\n",
    "workflow.add_node(\"model\", call_model)\n",
    "\n",
    "# Adding memory is straight forward in langgraph!\n",
    "memory = MemorySaver()\n",
    "\n",
    "app = workflow.compile(\n",
    "    checkpointer=memory\n",
    ")\n",
    "\n",
    "# The thread id is a unique key that identifies\n",
    "# this particular conversation.\n",
    "# We'll just generate a random uuid here.\n",
    "# This enables a single application to manage conversations among multiple users.\n",
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "input_message = HumanMessage(content=\"hi! I'm bob\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "input_message = HumanMessage(content=\"what is 1+1\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()\n",
    "\n",
    "# Here, let's confirm that the AI remembers our name!\n",
    "input_message = HumanMessage(content=\"what was my name?\")\n",
    "for event in app.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IdeaPad\\AppData\\Local\\Temp\\ipykernel_10592\\1324422065.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "memory = ConversationBufferWindowMemory(k=1) \n",
    "\n",
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationTokenBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'AI: Beautiful!\\nHuman: Chatbots are what?\\nAI: Charming!'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=20)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConversationSummaryMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IdeaPad\\AppData\\Local\\Temp\\ipykernel_10592\\3290462759.py:12: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human and AI are engaging in casual conversation. The human greeted the AI, stated they weren't doing much, and then asked about the schedule for the day.\n",
      "AI: There is a meeting at 8am with your product team. You will need your powerpoint presentation prepared. 9am-12pm have time to work on your LangChain project which will go quickly because Langchain is such a powerful tool. At Noon, lunch at the italian resturant with a customer who is driving from over an hour away to meet you to understand the latest in AI. Be sure to bring your laptop to show the latest LLM demo.\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Oh, excellent question! Considering your customer is coming specifically to understand the latest in AI and is traveling a significant distance, you want to *wow* them. Several demos spring to mind, depending on what aspects of AI you think will resonate most with their business. Let me break down a few options, with details on complexity and impact.\\n\\n**Option 1: The Document Q&A Demo (Relatively Easy to Set Up - High Impact)**\\n\\nThis is a fantastic starting point. It directly addresses a common business pain point: information overload. You can use LangChain to connect to a set of your companyâ€™s documents (think reports, FAQs, internal knowledge base) and allow the customer to ask questions in natural language. The LLM then retrieves relevant information and provides concise answers. \\n\\n*   **Tech Stack:** LangChain, a vector database (ChromaDB is a good, easy-to-use option), and an LLM like Gemini or GPT-3.5.\\n*   **Why it\\'s good:** Demonstrates practical application of LLMs, shows how AI can unlock value from existing data, and is relatively quick to build a basic version.\\n*   **Preparation:** You\\'ll need to ingest some sample documents *relevant to their industry* if possible. Even a few pages of a whitepaper would be great.\\n\\n**Option 2: The Personalized Email/Content Generator (Medium Complexity - High Impact)**\\n\\nThis demo showcases the power of LLMs for content creation. You could show how an LLM can generate personalized email drafts, marketing copy, or even product descriptions based on a few key inputs (customer segment, product features, desired tone).\\n\\n*   **Tech Stack:** LangChain, an LLM (GPT-4 would be ideal for quality, but 3.5 is fine for a demo), and potentially a simple UI for inputting parameters.\\n*   **Why it\\'s good:** Highlights AI\\'s ability to automate tasks, improve marketing effectiveness, and personalize customer experiences.\\n*   **Preparation:** Have a few example scenarios prepared â€“ e.g., \"Generate an email to a CFO explaining the ROI of our AI solution,\" or \"Write a product description for a new widget targeting small businesses.\"\\n\\n**Option 3: The Code Generation Demo (More Complex - Niche Impact)**\\n\\nIf your customer is technically inclined or works in a software-heavy industry, a code generation demo could be impressive. You can use an LLM to generate code snippets in various languages based on natural language instructions.\\n\\n*   **Tech Stack:** LangChain, an LLM (Code Llama is specifically designed for code generation), and a code editor to display the output.\\n*   **Why it\\'s good:** Demonstrates AI\\'s potential to accelerate software development and reduce coding errors.\\n*   **Preparation:** Have a few simple coding tasks ready â€“ e.g., \"Write a Python function to calculate the factorial of a number,\" or \"Generate HTML code for a basic form.\"\\n\\n**My Recommendation:**\\n\\nGiven the context â€“ a customer traveling a long distance to understand AI generally â€“ I\\'d strongly recommend the **Document Q&A Demo (Option 1)**. It\\'s the most broadly applicable, easiest to understand, and provides a tangible demonstration of AI\\'s value. It\\'s also less likely to get bogged down in technical details.\\n\\nBefore you decide, quickly think about what *their* business does. If they\\'re in marketing, the content generator might be better. If they\\'re a software company, the code generation demo could be a hit.\\n\\n\\n\\nDo any of those sound particularly appealing? Or tell me a little more about your customerâ€™s industry, and I can refine my recommendation! I can even help you brainstorm specific prompts to use during the demo.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})\n",
    "\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"System: The human and AI are engaging in casual conversation that has turned into work-related planning. The human greeted the AI, stated they weren't doing much, and asked about the day's schedule. The AI detailed a busy schedule including a product team meeting, dedicated time for a LangChain project, and a lunch meeting with a customer traveling a significant distance to learn about AI. The AI then provided three detailed options for an LLM demo to impress the customer â€“ a Document Q&A demo, a Personalized Email/Content Generator, and a Code Generation demo â€“ outlining the tech stack, benefits, preparation needed, and complexity of each. The AI recommended the Document Q&A demo as the most broadly applicable and easy to understand, but offered to refine the recommendation based on the customerâ€™s industry and even help brainstorm prompts. The AI then asked the human which demo option sounds most appealing or for more information about the customerâ€™s business.\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
