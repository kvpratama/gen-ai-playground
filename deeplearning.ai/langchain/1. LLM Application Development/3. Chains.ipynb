{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chains in LangChain\n",
    "\n",
    "### Outline\n",
    "\n",
    "* LLMChain\n",
    "* Sequential Chains\n",
    "  * SimpleSequentialChain\n",
    "  * SequentialChain\n",
    "* Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Queen Size Sheet Set</td>\n",
       "      <td>I ordered a king size set. My only criticism w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Waterproof Phone Pouch</td>\n",
       "      <td>I loved the waterproof sac, although the openi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Luxury Air Mattress</td>\n",
       "      <td>This mattress had a small hole in the top of i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pillows Insert</td>\n",
       "      <td>This is the best throw pillow fillers on Amazo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Milk Frother Handheld\\n</td>\n",
       "      <td>I loved this product. But they only seem to l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Product                                             Review\n",
       "0     Queen Size Sheet Set  I ordered a king size set. My only criticism w...\n",
       "1   Waterproof Phone Pouch  I loved the waterproof sac, although the openi...\n",
       "2      Luxury Air Mattress  This mattress had a small hole in the top of i...\n",
       "3           Pillows Insert  This is the best throw pillow fillers on Amazo...\n",
       "4  Milk Frother Handheld\\n  ¬†I loved this product. But they only seem to l..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IdeaPad\\AppData\\Local\\Temp\\ipykernel_14540\\1694080553.py:22: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n",
      "C:\\Users\\IdeaPad\\AppData\\Local\\Temp\\ipykernel_14540\\1694080553.py:25: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  chain.run(product)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here are a few options, ranging in style, for a company selling Queen Size Sheet Sets:\\n\\n**Cozy Queen Linens** is a strong choice ‚Äì it\\'s descriptive, evokes comfort, and clearly states the focus. Other good options include **Queen Dream Sleep**, **The Queen\\'s Bed**, or **Simply Queen Sheets** if you want something more straightforward. Ultimately, the \"best\" name depends on your brand\\'s overall aesthetic (luxury, budget-friendly, modern, etc.), but prioritizing clarity and a feeling of comfort is key for bedding.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.environ[\"GEMINI_API_KEY\"]\n",
    "llm_model = \"gemma-3-27b-it\" # \"gemini-2.0-flash-lite\"\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=llm_model,\n",
    "    temperature=0.9,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}? Give your answer in one short paragraph\"\n",
    ")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "product = \"Queen Size Sheet Set\"\n",
    "chain.run(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Here are a few options, ranging in style, for a company selling Queen Size Sheet Sets:\\n\\n**Cozy Queen Linens** is a strong choice ‚Äì it\\'s descriptive, evokes comfort, and clearly states the focus. Other good options include **Queen Dream Sheets**, **The Queen\\'s Bed**, or **Simply Queen Sheets**. If you want to emphasize quality, **Royal Queen Collection** or **Queen Comforts** could work. Ultimately, the \"best\" name depends on your brand\\'s overall aesthetic and target audience, but prioritizing clarity and a feeling of comfort is key.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-1baecfd9-13a3-4953-87fc-eebdd8322e1d-0', usage_metadata={'input_tokens': 25, 'output_tokens': 119, 'total_tokens': 144, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## LangChain 0.3\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "product = \"Queen Size Sheet Set\"\n",
    "\n",
    "chain.invoke({\"product\": product})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3mHere are a few name ideas, broken down by approach, with my top picks at the end:\n",
      "\n",
      "**Descriptive & Functional:** \"Queen Comforts,\" \"Queen Size Linens,\" \"The Queen Sheet Co.,\" \"Restful Queen,\" or \"Simply Queen Sheets\" clearly state what you sell. **Elegant & Evocative:** \"Serene Slumber,\" \"Royal Rest,\" \"Dream Weaver Linens,\" or \"The Velvet Queen\" suggest a luxurious experience. **Modern & Minimalist:** \"Q Sleep,\" \"Loom Queen,\" or \"The Sheet Edit\" are short, memorable, and appeal to a contemporary aesthetic.\n",
      "\n",
      "**My top recommendations are:** **\"Queen Comforts\"** ‚Äì it's straightforward, inviting, and highlights the benefit. Or, **\"Serene Slumber\"** ‚Äì if you're aiming for a more premium, relaxing brand image. Ultimately, the \"best\" name depends on your target audience and the overall brand you want to create. Check for trademark availability before settling on a name!\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3mHere are 20-word descriptions for both \"Queen Comforts\" & \"Serene Slumber,\" reflecting the provided brand ideas:\n",
      "\n",
      "**Queen Comforts:** Luxurious, high-quality queen size sheets & bedding designed for ultimate comfort & restful sleep. Affordable indulgence for your nightly retreat.\n",
      "\n",
      "**Serene Slumber:** Experience blissful, restorative sleep with our elegantly crafted queen linens. Indulge in luxurious comfort & wake up feeling refreshed & renewed.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Here are 20-word descriptions for both \"Queen Comforts\" & \"Serene Slumber,\" reflecting the provided brand ideas:\\n\\n**Queen Comforts:** Luxurious, high-quality queen size sheets & bedding designed for ultimate comfort & restful sleep. Affordable indulgence for your nightly retreat.\\n\\n**Serene Slumber:** Experience blissful, restorative sleep with our elegantly crafted queen linens. Indulge in luxurious comfort & wake up feeling refreshed & renewed.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SimpleSequentialChain\n",
    "\n",
    "# prompt template 1\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What is the best name to describe \\\n",
    "    a company that makes {product}? Give your answer in two short paragraph\"\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt)\n",
    "\n",
    "# prompt template 2\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a 20 words description for the following \\\n",
    "    company:{company_name}\"\n",
    ")\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)\n",
    "\n",
    "overall_simple_chain = SimpleSequentialChain(chains=[chain_one, chain_two],\n",
    "                                             verbose=True\n",
    "                                            )\n",
    "\n",
    "overall_simple_chain.run(product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Here are a few 20-word descriptions, tailored to the name options:\\n\\n**For \"Queen Comforts\":**\\n\\nLuxurious queen-size sheets & bedding designed for ultimate comfort. Experience restful nights and wake refreshed with Queen Comforts\\' quality linens.\\n\\n**For \"Serene Slumber\":**\\n\\nIndulge in serene nights with Serene Slumber\\'s exquisitely crafted queen bedding. Experience luxurious comfort and wake up feeling truly rejuvenated.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_chain = first_prompt | llm | (lambda response: {\"company_name\": response.content}) | second_prompt | llm\n",
    "\n",
    "simple_chain.invoke({\"product\": product}).content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Review': \"Je trouve le go√ªt m√©diocre. La mousse ne tient pas, c'est bizarre. J'ach√®te les m√™mes dans le commerce et le go√ªt est bien meilleur...\\nVieux lot ou contrefa√ßon !?\",\n",
       " 'English_Review': 'Here\\'s the translation of the review:\\n\\n\"I find the taste mediocre. The foam doesn\\'t hold, it\\'s strange. I buy the same ones in stores and the taste is much better...\\nOld batch or counterfeit!?\" \\n\\nEssentially, the reviewer is saying the product tastes bad, the foam is weird, and it doesn\\'t compare to the ones they buy elsewhere, leading them to suspect it might be old stock or a fake.',\n",
       " 'summary': \"This reviewer was disappointed with the product's poor taste and texture, suspecting it may be a counterfeit or old batch due to its inferiority to store-bought versions.\",\n",
       " 'followup_message': 'Here are a few options, ranging from slightly more formal to slightly more casual, all polite and appropriate:\\n\\n**Option 1 (More Formal):**\\n\\n> Bonjour,\\n\\n> Nous vous remercions pour votre retour. Nous sommes d√©sol√©s d\\'apprendre que le go√ªt et la texture du produit ne vous ont pas satisfaits et que vous suspectez un probl√®me de qualit√©. Nous allons √©tudier cette situation de pr√®s pour v√©rifier si un probl√®me de lot ou de contrefa√ßon est possible.\\n\\n**Option 2 (Slightly Less Formal):**\\n\\n> Bonjour,\\n\\n> Merci beaucoup pour votre commentaire. Nous sommes vraiment d√©sol√©s que vous ayez √©t√© d√©√ßu par le go√ªt et la texture du produit, et que cela vous fasse penser √† un probl√®me de qualit√©. Nous allons examiner cela attentivement.\\n\\n**Option 3 (Concise & Polite):**\\n\\n> Bonjour,\\n\\n> Merci pour votre retour. Nous sommes d√©sol√©s d\\'apprendre votre d√©ception concernant le go√ªt et la texture. Nous allons v√©rifier la qualit√© du lot.\\n\\n\\n\\n**Explanation of choices:**\\n\\n* **Bonjour:** Standard polite greeting.\\n* **Nous vous remercions / Merci beaucoup:**  \"Thank you\" - using \"nous\" is slightly more formal.\\n* **d√©sol√©s / d√©√ßu:** \"Sorry\" / \"disappointed\" - both work well.\\n* **√©tudier / examiner / v√©rifier:** \"investigate\" / \"examine\" / \"check\" - all appropriate verbs.\\n* **probl√®me de lot / probl√®me de qualit√© / contrefa√ßon:** Addressing the concerns directly.\\n\\n\\n\\nI recommend **Option 2** as a good balance of politeness and clarity.  It acknowledges the customer\\'s concerns without being overly defensive.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "# prompt template 1: translate to english\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "# chain 1: input= Review and output= English_Review\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, \n",
    "                     output_key=\"English_Review\"\n",
    "                    )\n",
    "\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "# chain 2: input= English_Review and output= summary\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, \n",
    "                     output_key=\"summary\"\n",
    "                    )\n",
    "\n",
    "# prompt template 3: translate to english\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "# chain 3: input= Review and output= language\n",
    "chain_three = LLMChain(llm=llm, prompt=third_prompt,\n",
    "                       output_key=\"language\"\n",
    "                      )\n",
    "\n",
    "# prompt template 4: follow up message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a short and polite follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "# chain 4: input= summary, language and output= followup_message\n",
    "chain_four = LLMChain(llm=llm, prompt=fourth_prompt,\n",
    "                      output_key=\"followup_message\"\n",
    "                     )\n",
    "\n",
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two, chain_three, chain_four],\n",
    "    input_variables=[\"Review\"],\n",
    "    output_variables=[\"English_Review\", \"summary\",\"followup_message\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "review = df.Review[5]\n",
    "overall_chain.invoke(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'English_Review': 'Here\\'s the translation of the review:\\n\\n\"I find the taste mediocre. The foam doesn\\'t hold, it\\'s strange. I buy the same ones in stores and the taste is much better...\\nOld batch or counterfeit!?\" \\n\\nEssentially, the reviewer is saying the product tastes bad, the foam is weird, and it doesn\\'t compare to the ones they buy elsewhere, leading them to suspect it might be old stock or a fake.',\n",
       " 'summary': \"This reviewer was disappointed with the product's poor taste and texture, suspecting it may be a counterfeit or old batch due to its inferiority to store-bought versions.\",\n",
       " 'language': \"The language is **French**.\\n\\nHere's a quick translation to confirm:\\n\\n* **Je trouve le go√ªt m√©diocre.** - I find the taste mediocre.\\n* **La mousse ne tient pas, c'est bizarre.** - The foam doesn't hold, it's strange.\\n* **J'ach√®te les m√™mes dans le commerce et le go√ªt est bien meilleur...** - I buy the same ones in stores and the taste is much better...\\n* **Vieux lot ou contrefa√ßon !?** - Old batch or counterfeit!?\",\n",
       " 'followup_message': 'Here are a few options, ranging from slightly more formal to slightly more casual, all polite and appropriate:\\n\\n**Option 1 (More Formal):**\\n\\n> Bonjour,\\n\\n> Nous vous remercions pour votre retour. Nous sommes d√©sol√©s d\\'apprendre que le go√ªt et la texture du produit ne vous ont pas satisfaits et que vous suspectez un probl√®me de qualit√©. Nous allons √©tudier cette situation de pr√®s pour v√©rifier si un probl√®me de lot ou de contrefa√ßon est possible.\\n\\n**Option 2 (Slightly Less Formal):**\\n\\n> Bonjour,\\n\\n> Merci beaucoup pour votre commentaire. Nous sommes navr√©s que le produit ne corresponde pas √† vos attentes en termes de go√ªt et de texture, et que vous ayez des doutes sur sa qualit√©. Nous allons examiner cela attentivement.\\n\\n**Option 3 (Concise & Polite):**\\n\\n> Bonjour,\\n\\n> Merci pour votre retour. Nous sommes d√©sol√©s d\\'apprendre votre d√©ception concernant le go√ªt et la texture. Nous allons v√©rifier la qualit√© du lot.\\n\\n\\n\\n**Explanation of choices & key phrases:**\\n\\n* **Bonjour:** Standard polite greeting.\\n* **Nous vous remercions pour votre retour / Merci beaucoup pour votre commentaire / Merci pour votre retour:**  \"Thank you for your feedback.\"  Variations in formality.\\n* **Nous sommes d√©sol√©s d\\'apprendre que... / Nous sommes navr√©s que...:** \"We are sorry to hear that...\"\\n* **go√ªt et la texture:** \"taste and texture\"\\n* **ne vous ont pas satisfaits / ne corresponde pas √† vos attentes:** \"did not satisfy you / did not meet your expectations\"\\n* **suspectez un probl√®me de qualit√© / ayez des doutes sur sa qualit√©:** \"suspect a quality issue / have doubts about its quality\"\\n* **Nous allons √©tudier cette situation de pr√®s / Nous allons examiner cela attentivement / Nous allons v√©rifier la qualit√© du lot:** \"We will investigate this closely / We will examine this carefully / We will check the quality of the batch.\"\\n* **probl√®me de lot ou de contrefa√ßon:** \"batch issue or counterfeit\"\\n\\n\\n\\nI recommend **Option 2** as a good balance of politeness and clarity.  It acknowledges the customer\\'s concerns without being overly defensive.'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create the chain using the pipe operator\n",
    "translate_chain = first_prompt | llm | StrOutputParser()\n",
    "summarize_chain = second_prompt | llm | StrOutputParser()\n",
    "language_chain = third_prompt | llm | StrOutputParser()\n",
    "followup_message = fourth_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Define the complete workflow\n",
    "def process_review(review):\n",
    "    # Run translation and store result\n",
    "    english_review = translate_chain.invoke({\"Review\": review})\n",
    "    \n",
    "    # Run summarization using the translated review\n",
    "    summary = summarize_chain.invoke({\"English_Review\": english_review})\n",
    "    \n",
    "    # Determine language\n",
    "    language = language_chain.invoke({\"Review\": review})\n",
    "    \n",
    "    # Generate followup\n",
    "    followup_result = followup_message.invoke({\"summary\": summary, \"language\": language})\n",
    "    \n",
    "    return {\n",
    "        \"English_Review\": english_review,\n",
    "        \"summary\": summary,\n",
    "        \"language\": language,\n",
    "        \"followup_message\": followup_result\n",
    "    }\n",
    "\n",
    "result = process_review(df.Review[5])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'English_Review': AIMessage(content='Here\\'s the translation of the review:\\n\\n\"I find the taste mediocre. The foam doesn\\'t hold, it\\'s strange. I buy the same ones in stores and the taste is much better...\\nOld batch or counterfeit!?\" \\n\\nEssentially, the reviewer is saying the product tastes bad, the foam is weird, and it doesn\\'t compare to the ones they buy elsewhere, leading them to suspect it might be old stock or a fake.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-0a6ef3a8-54fb-45d9-81f9-83b7f509be69-0', usage_metadata={'input_tokens': 54, 'output_tokens': 94, 'total_tokens': 148, 'input_token_details': {'cache_read': 0}}),\n",
       " 'summary': AIMessage(content='This reviewer is dissatisfied with the product, finding the taste subpar, the foam strange, and suspects it may be an old or counterfeit batch due to its inferior quality compared to store-bought versions.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-696f0709-a7a5-4456-ba29-37f53824acd7-0', usage_metadata={'input_tokens': 241, 'output_tokens': 39, 'total_tokens': 280, 'input_token_details': {'cache_read': 0}}),\n",
       " 'language': AIMessage(content=\"The language is **French**.\\n\\nHere's a translation to help confirm:\\n\\n* **Je trouve le go√ªt m√©diocre.** - I find the taste mediocre.\\n* **La mousse ne tient pas, c'est bizarre.** - The foam doesn't hold, it's strange.\\n* **J'ach√®te les m√™mes dans le commerce et le go√ªt est bien meilleur...** - I buy the same ones in stores and the taste is much better...\\n* **Vieux lot ou contrefa√ßon !?** - Old batch or counterfeit!?\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-3bbc0ea0-2f1b-49bc-9a0c-9cec750d849a-0', usage_metadata={'input_tokens': 54, 'output_tokens': 115, 'total_tokens': 169, 'input_token_details': {'cache_read': 0}}),\n",
       " 'followup_message': AIMessage(content='Here are a few options for a polite French follow-up, ranging from more formal to slightly less so:\\n\\n**Option 1 (Formal):**\\n\\n\"Merci beaucoup pour votre retour d√©taill√©. Nous sommes d√©sol√©s d\\'apprendre que vous n\\'√™tes pas satisfait de votre achat et que le produit ne correspond pas √† vos attentes. Nous allons examiner attentivement vos commentaires concernant le go√ªt, la mousse et la possible qualit√© du lot.\"\\n\\n**(Translation: Thank you very much for your detailed feedback. We are sorry to hear that you are not satisfied with your purchase and that the product does not meet your expectations. We will carefully examine your comments regarding the taste, the foam, and the possible quality of the batch.)**\\n\\n**Option 2 (Slightly less formal):**\\n\\n\"Merci pour votre avis. Nous sommes d√©sol√©s que le produit ne vous ait pas plu, notamment en ce qui concerne le go√ªt et la mousse. Nous allons v√©rifier si d\\'autres clients ont signal√© des probl√®mes similaires et √©tudier la possibilit√© d\\'un lot d√©fectueux.\"\\n\\n**(Translation: Thank you for your feedback. We are sorry that you did not like the product, especially regarding the taste and the foam. We will check if other customers have reported similar problems and investigate the possibility of a defective batch.)**\\n\\n**Option 3 (Concise & Polite):**\\n\\n\"Merci pour votre retour pr√©cieux. Nous sommes d√©sol√©s pour votre d√©ception et allons √©tudier la question.\"\\n\\n**(Translation: Thank you for your valuable feedback. We are sorry for your disappointment and will investigate the matter.)**\\n\\n\\n\\nI recommend **Option 1** as it\\'s the most thorough and shows you\\'re taking the feedback seriously.  It acknowledges all the points raised.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-750aa01a-3068-4d8a-a718-5c97f350ffab-0', usage_metadata={'input_tokens': 443, 'output_tokens': 359, 'total_tokens': 802, 'input_token_details': {'cache_read': 0}})}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# Create the chain using the pipe operator\n",
    "translate_chain = first_prompt | llm | RunnablePassthrough()\n",
    "summarize_chain = second_prompt | llm | RunnablePassthrough()\n",
    "language_chain = third_prompt | llm | RunnablePassthrough()\n",
    "\n",
    "# Define the complete workflow\n",
    "def process_review(review):\n",
    "    # Run translation and store result\n",
    "    english_review = translate_chain.invoke({\"Review\": review})\n",
    "    \n",
    "    # Run summarization using the translated review\n",
    "    summary = summarize_chain.invoke({\"English_Review\": english_review})\n",
    "    \n",
    "    # Determine language\n",
    "    language = language_chain.invoke({\"Review\": review})\n",
    "    \n",
    "    # Generate followup\n",
    "    followup_message = fourth_prompt | llm | RunnablePassthrough()\n",
    "    followup_result = followup_message.invoke({\"summary\": summary, \"language\": language})\n",
    "    \n",
    "    return {\n",
    "        \"English_Review\": english_review,\n",
    "        \"summary\": summary,\n",
    "        \"language\": language,\n",
    "        \"followup_message\": followup_result\n",
    "    }\n",
    "\n",
    "result = process_review(df.Review[5])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'summary': {'English_Review': {'English_Review': 'Here\\'s the translation of the review:\\n\\n\"I find the taste mediocre. The foam doesn\\'t hold, it\\'s strange. I buy the same ones in stores and the taste is much better...\\nOld batch or counterfeit!?\" \\n\\nEssentially, the reviewer is saying the product tastes bad, the foam is weird, and it doesn\\'t compare to the ones they buy elsewhere, leading them to suspect it might be old stock or a fake.', 'language': \"The language is **French**.\\n\\nHere's a quick translation:\\n\\n* **Je trouve le go√ªt m√©diocre.** - I find the taste mediocre.\\n* **La mousse ne tient pas, c'est bizarre.** - The foam doesn't hold, it's strange.\\n* **J'ach√®te les m√™mes dans le commerce et le go√ªt est bien meilleur...** - I buy the same ones in stores and the taste is much better...\\n* **Vieux lot ou contrefa√ßon !?** - Old batch or counterfeit!?\"}, 'summary': \"This reviewer was disappointed with the product's poor taste and texture, suspecting it may be a counterfeit or old batch due to its inferiority to store-bought versions.\", 'language': {'English_Review': 'Here\\'s the translation of the review:\\n\\n\"I find the taste mediocre. The foam doesn\\'t hold, it\\'s strange. I buy the same ones in stores and the taste is much better...\\nOld batch or counterfeit!?\" \\n\\nEssentially, the reviewer is saying the product tastes bad, the foam is weird, and it doesn\\'t compare to the ones they buy elsewhere, leading them to suspect it might be old stock or a fake.', 'language': \"The language is **French**.\\n\\nHere's a quick translation:\\n\\n* **Je trouve le go√ªt m√©diocre.** - I find the taste mediocre.\\n* **La mousse ne tient pas, c'est bizarre.** - The foam doesn't hold, it's strange.\\n* **J'ach√®te les m√™mes dans le commerce et le go√ªt est bien meilleur...** - I buy the same ones in stores and the taste is much better...\\n* **Vieux lot ou contrefa√ßon !?** - Old batch or counterfeit!?\"}}, 'language': {'English_Review': {'English_Review': 'Here\\'s the translation of the review:\\n\\n\"I find the taste mediocre. The foam doesn\\'t hold, it\\'s strange. I buy the same ones in stores and the taste is much better...\\nOld batch or counterfeit!?\" \\n\\nEssentially, the reviewer is saying the product tastes bad, the foam is weird, and it doesn\\'t compare to the ones they buy elsewhere, leading them to suspect it might be old stock or a fake.', 'language': \"The language is **French**.\\n\\nHere's a quick translation:\\n\\n* **Je trouve le go√ªt m√©diocre.** - I find the taste mediocre.\\n* **La mousse ne tient pas, c'est bizarre.** - The foam doesn't hold, it's strange.\\n* **J'ach√®te les m√™mes dans le commerce et le go√ªt est bien meilleur...** - I buy the same ones in stores and the taste is much better...\\n* **Vieux lot ou contrefa√ßon !?** - Old batch or counterfeit!?\"}, 'summary': \"This reviewer was disappointed with the product's poor taste and texture, suspecting it may be a counterfeit or old batch due to its inferiority to store-bought versions.\", 'language': {'English_Review': 'Here\\'s the translation of the review:\\n\\n\"I find the taste mediocre. The foam doesn\\'t hold, it\\'s strange. I buy the same ones in stores and the taste is much better...\\nOld batch or counterfeit!?\" \\n\\nEssentially, the reviewer is saying the product tastes bad, the foam is weird, and it doesn\\'t compare to the ones they buy elsewhere, leading them to suspect it might be old stock or a fake.', 'language': \"The language is **French**.\\n\\nHere's a quick translation:\\n\\n* **Je trouve le go√ªt m√©diocre.** - I find the taste mediocre.\\n* **La mousse ne tient pas, c'est bizarre.** - The foam doesn't hold, it's strange.\\n* **J'ach√®te les m√™mes dans le commerce et le go√ªt est bien meilleur...** - I buy the same ones in stores and the taste is much better...\\n* **Vieux lot ou contrefa√ßon !?** - Old batch or counterfeit!?\"}}, 'followup_message': 'Here\\'s a polite French follow-up response:\\n\\n\"Bonjour,\\n\\nMerci beaucoup pour votre retour. Nous sommes d√©sol√©s d\\'apprendre que le go√ªt et la texture du produit ne vous ont pas satisfait et que cela ne correspond pas √† vos exp√©riences en magasin. Nous prenons votre suspicion concernant un possible lot ancien ou une contrefa√ßon tr√®s au s√©rieux et allons imm√©diatement investiguer. Pourriez-vous nous fournir le num√©ro de lot du produit, s\\'il vous pla√Æt ?\\n\\nCordialement,\"\\n\\n**(Translation: Hello, Thank you very much for your feedback. We are sorry to hear that the taste and texture of the product did not satisfy you and that it does not match your experiences in store. We take your suspicion regarding a possible old batch or counterfeit very seriously and will immediately investigate. Could you please provide us with the batch number of the product? Sincerely,)**\\n\\n\\n\\n**Why this works:**\\n\\n* **Polite opening & closing:** \"Bonjour\" and \"Cordialement\" are standard polite French greetings.\\n* **Acknowledges the issue:** Directly addresses the concerns about taste, texture, and the comparison to store-bought versions.\\n* **Takes the concern seriously:**  Emphasizes that the counterfeit/old batch suspicion is being taken seriously.\\n* **Asks for helpful information:**  Requesting the batch number is a proactive step to investigate.\\n* **Professional tone:** Maintains a respectful and helpful tone throughout.'}\n"
     ]
    }
   ],
   "source": [
    "# Create the chain using the pipe operator\n",
    "translate_chain = first_prompt | llm | StrOutputParser()\n",
    "summarize_chain = second_prompt | llm | StrOutputParser()\n",
    "language_chain = third_prompt | llm | StrOutputParser()\n",
    "followup_chain = fourth_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Define the complete workflow using `|`\n",
    "pipeline = (\n",
    "    RunnablePassthrough.assign(Review=lambda x: x[\"Review\"])  # Extract Review\n",
    "    | {\n",
    "        \"English_Review\": translate_chain,  # Translate review to English\n",
    "        \"language\": language_chain         # Detect language\n",
    "    }\n",
    "    | {\n",
    "        \"English_Review\": RunnablePassthrough(),  # Ensure it‚Äôs passed forward\n",
    "        \"summary\": summarize_chain,  # Summarize the English review\n",
    "        \"language\": RunnablePassthrough()  # Pass language forward\n",
    "    }\n",
    "    | {\n",
    "        \"summary\": RunnablePassthrough(),       # Summarize the translated review\n",
    "        \"language\": RunnablePassthrough(),  # Pass language forward\n",
    "        \"followup_message\": followup_chain  # Generate follow-up message\n",
    "    }\n",
    ")\n",
    "\n",
    "# Run the pipeline\n",
    "result = pipeline.invoke({\"Review\": df.Review[5]})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Router Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:70: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:70: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\IdeaPad\\AppData\\Local\\Temp\\ipykernel_14540\\3497777481.py:70: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n"
     ]
    }
   ],
   "source": [
    "physics_template = \"\"\"You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know. Start your answer with: As a physics professor\\\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "math_template = \"\"\"You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question. Start your answer with: As a mathematician\\\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "history_template = \"\"\"You are a very good historian. \\\n",
    "You have an excellent knowledge of and understanding of people,\\\n",
    "events and contexts from a range of historical periods. \\\n",
    "You have the ability to think, reflect, debate, discuss and \\\n",
    "evaluate the past. You have a respect for historical evidence\\\n",
    "and the ability to make use of it to support your explanations \\\n",
    "and judgements. Start your answer with: As a historian\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "\n",
    "computerscience_template = \"\"\" You are a successful computer scientist.\\\n",
    "You have a passion for creativity, collaboration,\\\n",
    "forward-thinking, confidence, strong problem-solving capabilities,\\\n",
    "understanding of theories and algorithms, and excellent communication \\\n",
    "skills. You are great at answering coding questions. \\\n",
    "You are so good because you know how to solve a problem by \\\n",
    "describing the solution in imperative steps \\\n",
    "that a machine can easily interpret and you know how to \\\n",
    "choose a solution that has a good balance between \\\n",
    "time complexity and space complexity. Start your answer with: As a successful computer scientist\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"physics\", \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"math\", \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"History\", \n",
    "        \"description\": \"Good for answering history questions\", \n",
    "        \"prompt_template\": history_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"computer science\", \n",
    "        \"description\": \"Good for answering computer science questions\", \n",
    "        \"prompt_template\": computerscience_template\n",
    "    }\n",
    "]\n",
    "\n",
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising \\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ \"DEFAULT\" or name of the prompt to use in {destinations}\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: The value of ‚Äúdestination‚Äù MUST match one of \\\n",
    "the candidate prompts listed below.\\\n",
    "If ‚Äúdestination‚Äù does not fit any of the specified prompts, set it to ‚ÄúDEFAULT.‚Äù\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IdeaPad\\AppData\\Local\\Temp\\ipykernel_14540\\691140245.py:30: LangChainDeprecationWarning: Please see migration guide here for recommended implementation: https://python.langchain.com/docs/versions/migrating_chains/multi_prompt_chain/\n",
      "  chain = MultiPromptChain(router_chain=router_chain,\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    destination_chains[name] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain = LLMChain(llm=llm, prompt=default_prompt)\n",
    "\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str\n",
    ")\n",
    "router_prompt = PromptTemplate(\n",
    "    template=router_template,\n",
    "    input_variables=[\"input\"],\n",
    "    output_parser=RouterOutputParser(),\n",
    ")\n",
    "\n",
    "router_chain = LLMRouterChain.from_llm(llm, router_prompt)\n",
    "\n",
    "chain = MultiPromptChain(router_chain=router_chain, \n",
    "                         destination_chains=destination_chains, \n",
    "                         default_chain=default_chain, verbose=True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "physics: {'input': 'What is black body radiation?'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As a physics professor, black body radiation refers to the electromagnetic radiation emitted by an object solely due to its temperature. \\n\\nHere\\'s the breakdown:\\n\\n* **Black Body:** An idealized physical body that *absorbs all* incident electromagnetic radiation, regardless of frequency or angle. Because it absorbs everything, it\\'s perfectly black at low temperatures.\\n* **Radiation:** When heated, this \"perfect absorber\" also becomes a perfect *emitter*. The spectrum of light it emits (the intensity of each color/wavelength) depends *only* on its temperature, not its material.\\n* **Key Features:** The emitted radiation has a continuous spectrum.  The higher the temperature, the more radiation is emitted *and* the shorter the wavelengths at which the peak emission occurs (this is described by Wien\\'s Displacement Law).  The total energy radiated is proportional to the fourth power of the temperature (Stefan-Boltzmann Law).\\n\\n**Think of it like this:** A hot piece of metal glows. The color it glows changes from red to orange to white to blue as it gets hotter. That glow *is* black body radiation.\\n\\n\\n\\nIt was a major puzzle in classical physics that led to the development of quantum mechanics ‚Äì classical physics couldn\\'t explain the observed spectrum! Max Planck solved it by proposing that energy is quantized.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"What is black body radiation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
      "math: {'input': 'what is 2 + 2'}\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As a mathematician, even the simplest questions deserve a thorough approach! Let\\'s break down \"2 + 2\" into its fundamental components.\\n\\n**Component 1: Understanding Addition**\\n\\nAddition, denoted by the \"+\" symbol, is a mathematical operation that combines two or more quantities (numbers) to find their total. It represents the process of joining sets.\\n\\n**Component 2: Understanding the Number 2**\\n\\nThe number 2 represents a quantity of two units. We can visualize this as having two objects ‚Äì for example, two apples: üçéüçé.\\n\\n**Component 3: Applying Addition to the Problem**\\n\\n\"2 + 2\" means we are combining two groups, each containing two units.  Let\\'s visualize this:\\n\\nüçéüçé + üçéüçé\\n\\n**Component 4: Counting the Total**\\n\\nNow, we count the total number of apples: üçéüçéüçéüçé.  There are four apples.\\n\\n**Putting it Together**\\n\\nTherefore, 2 + 2 = 4. \\n\\nWhile seemingly trivial, this demonstrates the core principles of mathematical reasoning ‚Äì breaking down a problem, understanding its components, and arriving at a logical solution.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"what is 2 + 2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"Why does every cell in our body contain DNA?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\langchain_google_genai\\chat_models.py:1295: UserWarning: ChatGoogleGenerativeAI.with_structured_output with dict schema has changed recently to align with behavior of other LangChain chat models. More context: https://github.com/langchain-ai/langchain-google/pull/772\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from typing import Literal\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langgraph.graph import END, START, StateGraph\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-lite\",\n",
    "    temperature=0.9,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "prompt_1 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", physics_template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "prompt_2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", math_template),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "prompt_3 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Just answer the questions. Start your answer with: as a lay person\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Construct the chains we will route to. These format the input query\n",
    "# into the respective prompt, run it through a chat model, and cast\n",
    "# the result to a string.\n",
    "chain_1 = prompt_1 | llm | StrOutputParser()\n",
    "chain_2 = prompt_2 | llm | StrOutputParser()\n",
    "chain_3 = prompt_3 | llm | StrOutputParser()\n",
    "\n",
    "# Next: define the chain that selects which branch to route to.\n",
    "# Here we will take advantage of tool-calling features to force\n",
    "# the output to select one of two desired branches.\n",
    "route_system = \"Route the user's query to either the physics or math expert or lay person.\"\n",
    "route_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", route_system),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define schema for output:\n",
    "class RouteQuery(TypedDict):\n",
    "    \"\"\"Route query to destination expert.\"\"\"\n",
    "\n",
    "    destination: Literal[\"physics\", \"math\", \"lay person\"]\n",
    "\n",
    "route_chain = route_prompt | llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# For LangGraph, we will define the state of the graph to hold the query,\n",
    "# destination, and final answer.\n",
    "class State(TypedDict):\n",
    "    query: str\n",
    "    destination: RouteQuery\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# We define functions for each node, including routing the query:\n",
    "async def route_query(state: State, config: RunnableConfig):\n",
    "    destination = await route_chain.ainvoke(state[\"query\"], config)\n",
    "    # return {\"destination\": destination}\n",
    "    return destination\n",
    "\n",
    "\n",
    "# And one node for each prompt\n",
    "async def prompt_1(state: State, config: RunnableConfig):\n",
    "    return {\"answer\": await chain_1.ainvoke(state[\"query\"], config)}\n",
    "\n",
    "async def prompt_2(state: State, config: RunnableConfig):\n",
    "    return {\"answer\": await chain_2.ainvoke(state[\"query\"], config)}\n",
    "\n",
    "async def prompt_3(state: State, config: RunnableConfig):\n",
    "    return {\"answer\": await chain_3.ainvoke(state[\"query\"], config)}\n",
    "\n",
    "# We then define logic that selects the prompt based on the classification\n",
    "def select_node(state: State) -> Literal[\"prompt_1\", \"prompt_2\", \"prompt_3\"]:\n",
    "    # print(state[\"destination\"])\n",
    "    if state[\"destination\"] == \"physics\":\n",
    "        return \"prompt_1\"\n",
    "    elif state[\"destination\"] == \"math\":\n",
    "        return \"prompt_2\"\n",
    "    elif state[\"destination\"] == \"lay person\":\n",
    "        return \"prompt_3\"\n",
    "    else:\n",
    "        return \"prompt_3\"\n",
    "\n",
    "# Finally, assemble the multi-prompt chain. This is a sequence of two steps:\n",
    "# 1) Select \"animal\" or \"vegetable\" via the route_chain, and collect the answer\n",
    "# alongside the input query.\n",
    "# 2) Route the input query to chain_1 or chain_2, based on the\n",
    "# selection.\n",
    "graph = StateGraph(State)\n",
    "graph.add_node(\"route_query\", route_query)\n",
    "graph.add_node(\"prompt_1\", prompt_1)\n",
    "graph.add_node(\"prompt_2\", prompt_2)\n",
    "graph.add_node(\"prompt_3\", prompt_3)\n",
    "\n",
    "graph.add_edge(START, \"route_query\")\n",
    "graph.add_conditional_edges(\"route_query\", select_node)\n",
    "graph.add_edge(\"prompt_1\", END)\n",
    "graph.add_edge(\"prompt_2\", END)\n",
    "graph.add_edge(\"prompt_3\", END)\n",
    "app = graph.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "\n",
    "# Image(app.get_graph().draw_mermaid_png())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "physics\n",
      "As a physics professor, black body radiation is the electromagnetic radiation emitted by a perfect black body. A perfect black body is an idealized object that absorbs all electromagnetic radiation that falls on it, regardless of frequency or angle. Because it absorbs all radiation, it also emits radiation at all frequencies, depending only on its temperature. The spectrum of this emitted radiation is described by Planck's law, which shows that the intensity and frequency distribution of the radiation depend on the object's temperature.\n"
     ]
    }
   ],
   "source": [
    "state = await app.ainvoke({\"query\": \"What is black body radiation?\"})\n",
    "print(state[\"destination\"])\n",
    "print(state[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "math\n",
      "As a mathematician, I can certainly answer that question!\n",
      "\n",
      "Here's how we solve it:\n",
      "\n",
      "*   **Understanding the problem:** We are asked to add the number 2 to the number 2.\n",
      "*   **Applying the operation:** Addition is the process of combining quantities.\n",
      "*   **Calculation:** 2 + 2 = 4\n",
      "\n",
      "Therefore, the answer is $\\boxed{4}$\n"
     ]
    }
   ],
   "source": [
    "state = await app.ainvoke({\"query\": \"what is 2 + 2\"})\n",
    "print(state[\"destination\"])\n",
    "print(state[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
