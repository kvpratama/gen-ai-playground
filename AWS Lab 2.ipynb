{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune a Generative AI Model for Dialogue Summarization\n",
    "\n",
    "Fine-tune FLAN-T5 model from Hugging Face for enhanced dialogue summarization. Full fine-tuning and Parameter Efficient Fine-Tuning (PEFT) will be explored and evaluated with ROUGE metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\io\\image.py:14: UserWarning: Failed to load image Python extension: 'Could not find module 'C:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "import time\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "hf_dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(hf_dataset_name)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "# original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
    "# original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to('xpu')\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to('xpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to pull out the number of model parameters and find out how many of them are trainable. The following function can be used to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable model parameters 100.0%\n"
     ]
    }
   ],
   "source": [
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "    \n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters {trainable_model_params/all_model_params * 100}%\"\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(original_model)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Test the Model with Zero Shot Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summarize the following conversation.\n",
      "\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary:\n",
      "\n",
      "-------------------------------------------------\n",
      "Baseline Summary:\n",
      " #Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "-------------------------------------------------\n",
      "Model Generation - Zero Shot:\n",
      " #Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "\n",
    "dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "summary = dataset[\"test\"][index][\"summary\"]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "output = tokenizer.decode(original_model.generate(tokenizer(prompt, return_tensors=\"pt\")['input_ids'].to('xpu'), max_new_tokens=200)[0], skip_special_tokens=True)\n",
    "\n",
    "dash_line = \"-\".join(\"\" for i in range(50))\n",
    "print(prompt)\n",
    "print(dash_line)\n",
    "print(\"Baseline Summary:\\n\", summary)\n",
    "print(dash_line)\n",
    "print(\"Model Generation - Zero Shot:\\n\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Perform Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocess the Dialog-Summary Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    start_prompt = \"Summarize the following conversation.\\n\\n\"\n",
    "    end_prompt = \"\\n\\nSummary: \"\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    # prompt = start_prompt + example[\"dialogue\"] + end_prompt # when batched=False\n",
    "    example['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\")['input_ids'].to('xpu')\n",
    "    example['labels'] = tokenizer(example['summary'], padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "    return example\n",
    "\n",
    "# The dataset actually contains 3 diff split: train, validation, test.\n",
    "# The tokenize_function code is handling all data across all splits in batches\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['id', 'topic', 'dialogue', 'summary'])\n",
    "# print(tokenized_dataset['validation'][0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save some time, subsample the dataset:\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.filter(lambda example, index: index % 10 == 0, with_indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of dataset:\n",
      "Training: (1246, 2)\n",
      "Validation: (50, 2)\n",
      "Test: (150, 2)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1246\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes of dataset:\")\n",
    "print(f\"Training: {tokenized_dataset['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_dataset['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_dataset['test'].shape}\")\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Fine-Tune the Model with the Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = f\"./dialogue-summary-training-{str(int(time.time()))}\"\n",
    "\n",
    "# training_args = TrainingArguments(\n",
    "#     output_dir=output_dir,\n",
    "#     learning_rate=1e-5,\n",
    "#     num_train_epochs=1,\n",
    "#     weight_decay=0.01,\n",
    "#     logging_steps=1,\n",
    "#     max_steps=1\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=original_model,\n",
    "#     args=training_args,\n",
    "#     train_dataset=tokenized_dataset['train'],\n",
    "#     eval_dataset=tokenized_dataset['validation']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Evaluate the Model Quantitatively (with ROUGE Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\generation\\utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Human Baseline</th>\n",
       "      <th>Original Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you. #</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you. #</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>#Person1#: I need to take a dictation for you. #</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>The traffic jam at the Carrefour intersection ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>Masha and Hero are getting divorced.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>#Person1#: Happy Birthday, Brian. #Person2#: I'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Human Baseline  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                                      Original Model  \n",
       "0   #Person1#: I need to take a dictation for you. #  \n",
       "1   #Person1#: I need to take a dictation for you. #  \n",
       "2   #Person1#: I need to take a dictation for you. #  \n",
       "3  The traffic jam at the Carrefour intersection ...  \n",
       "4  The traffic jam at the Carrefour intersection ...  \n",
       "5  The traffic jam at the Carrefour intersection ...  \n",
       "6               Masha and Hero are getting divorced.  \n",
       "7               Masha and Hero are getting divorced.  \n",
       "8               Masha and Hero are getting divorced.  \n",
       "9    #Person1#: Happy Birthday, Brian. #Person2#: I'  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_baseline_summaries = []\n",
    "original_model_summaries = []\n",
    "\n",
    "for i in range(10):\n",
    "    human_baseline_summaries.append(dataset[\"test\"][i]['summary'])\n",
    "    prompt = f\"\"\"\n",
    "        Summarize the following conversation:\n",
    "        {dataset[\"test\"][i]['dialogue']}\n",
    "        Summary: \n",
    "\"\"\"\n",
    "    model_output = original_model.generate(tokenizer(prompt, return_tensors='pt')['input_ids'].to('xpu'))[0]\n",
    "    # original_model.generate(tokenizer(prompt, return_tensors=\"pt\")['input_ids'].to('xpu')\n",
    "    original_model_summaries.append(tokenizer.decode(model_output, skip_special_tokens=True))\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries))\n",
    "df = pd.DataFrame(zipped_summaries, columns=['Human Baseline', 'Original Model'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.3043025373670535, 'rouge2': 0.11228756210130228, 'rougeL': 0.2602913752913753, 'rougeLsum': 0.25733446959253414}\n"
     ]
    }
   ],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries, \n",
    "    references=human_baseline_summaries, \n",
    "    use_aggregator=True, \n",
    "    use_stemmer=True)\n",
    "\n",
    "print(original_model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Perform Parameter Efficient Fine-Tuning (PEFT)\n",
    "\n",
    "PEFT is a generic term tha includes **Low-Rank Adaptation (LoRA)** and prompt tuning (NOT THE SAME as prompt engineering!). LoRA at a very high level allows the user to fine-tune their model using fewer compute resources (in some cases a single GPU). After fine-tuning for a specific task, use case or tenant with LoRA, the result is that the original LLM remains unchange and a newly-trained \"LoRA adapter\" emerges. This LoRA adapter is much, much smaller than the original LLM (MBs vs GBs). \n",
    "\n",
    "That said, at inference time, the LoRA adapter needs to be reunited and combined with its original LLM to serve the inference request. The benefit is that many LoRA adapters can re-use the original LLM which reduces overall memory requirements when serving multiple tasks and use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Setup the PEFT/LoRA model for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32, # Rank,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add LoRA adapter  layers/parameters to the original LLM to be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters 1.4092820552029972%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(original_model, lora_config)\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train PEFT Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"./dialogue-summary-training-peft\"\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3, # Higher learning rate than full fine-tuning\n",
    "    num_train_epochs=1,\n",
    "    # weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    # max_steps=5,\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    # eval_dataset=tokenized_dataset['validation'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peft_trainer.train()\n",
    "peft_model_path = \"./dialogue-summary-training-peft/lora\"\n",
    "\n",
    "# peft_trainer.model.save_pretrained(peft_model_path)\n",
    "# tokenizer.save_pretrained(peft_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0\n",
      "all model parameters: 251116800\n",
      "percentage of trainable model parameters 0.0%\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype=torch.bfloat16).to('xpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(peft_model_base,\n",
    "                                       peft_model_path,\n",
    "                                       torch_dtype=torch.bfloat16,\n",
    "                                       is_trainable=False).to(\"xpu\")\n",
    "\n",
    "print(print_number_of_trainable_model_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Evaluate the Model Qualitatively (Human Evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person2# wants to upgrade his hardware. #Person1# wants to upgrade his computer to a CD-ROM drive.\n",
      "#Person2# wants to upgrade his hardware. #Person2# wants to add a painting program to his software. #Person2# wants to upgrade his hardware. #Person2# wants to add a CD-ROM drive.\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "        Summarize the following conversation:\n",
    "        {dataset['test'][index]['dialogue']}\n",
    "        Summary: \n",
    "\"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"xpu\")\n",
    "\n",
    "peft_base_outputs = tokenizer.decode(original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))[0], skip_special_tokens=True)\n",
    "peft_model_outputs = tokenizer.decode(peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))[0], skip_special_tokens=True)\n",
    "\n",
    "print(peft_base_outputs)\n",
    "print(peft_model_outputs)\n",
    "print(dataset['test'][index]['summary'])\n",
    "# output = tokenizer.decode(original_model.generate(tokenizer(prompt, return_tensors=\"pt\")['input_ids'].to('xpu'), max_new_tokens=200)[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\transformers\\generation\\utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Human Baseline</th>\n",
       "      <th>Original Model</th>\n",
       "      <th>PEFT Model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ms. Dawson helps #Person1# to write a memo to ...</td>\n",
       "      <td>@Person1# wants to take a dictation for me.</td>\n",
       "      <td>#Person2# wants to take a dictation for #Person2#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>In order to prevent employees from wasting tim...</td>\n",
       "      <td>#Person2# wants #Person2 to take a dictation f...</td>\n",
       "      <td>#Person2# wants to take a dictation for #Person2#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ms. Dawson takes a dictation for #Person1# abo...</td>\n",
       "      <td>@Person1# is a memo to all employees.</td>\n",
       "      <td>#Person2# wants to take a dictation for #Person2#</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person2# arrives late because of traffic jam....</td>\n",
       "      <td>#Person1# is stuck in traffic and a terrible t...</td>\n",
       "      <td>#Person1# got stuck in traffic and got stuck i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person2# decides to follow #Person1#'s sugges...</td>\n",
       "      <td>You're finally here!</td>\n",
       "      <td>#Person1# got stuck in traffic and got stuck i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>#Person2# complains to #Person1# about the tra...</td>\n",
       "      <td>#Person2# is stuck in traffic and he wants to ...</td>\n",
       "      <td>#Person1# got stuck in traffic and got stuck i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero get d...</td>\n",
       "      <td>You never believe what happened when Masha and...</td>\n",
       "      <td>#Person1# is getting divorced. #Person2# is su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>#Person1# tells Kate that Masha and Hero are g...</td>\n",
       "      <td>#Person1# and Hero are getting divorced. #Pers...</td>\n",
       "      <td>#Person1# is getting divorced. #Person2# is su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>#Person1# and Kate talk about the divorce betw...</td>\n",
       "      <td>@Person2# wants to divorce Masha and Hero. Mas...</td>\n",
       "      <td>#Person1# is getting divorced. #Person2# is su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>#Person1# and Brian are at the birthday party ...</td>\n",
       "      <td>#Person1# is a great party. #Person1# is happy to</td>\n",
       "      <td>#Person1# wishes Brian's birthday. #Person2# is a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Human Baseline  \\\n",
       "0  Ms. Dawson helps #Person1# to write a memo to ...   \n",
       "1  In order to prevent employees from wasting tim...   \n",
       "2  Ms. Dawson takes a dictation for #Person1# abo...   \n",
       "3  #Person2# arrives late because of traffic jam....   \n",
       "4  #Person2# decides to follow #Person1#'s sugges...   \n",
       "5  #Person2# complains to #Person1# about the tra...   \n",
       "6  #Person1# tells Kate that Masha and Hero get d...   \n",
       "7  #Person1# tells Kate that Masha and Hero are g...   \n",
       "8  #Person1# and Kate talk about the divorce betw...   \n",
       "9  #Person1# and Brian are at the birthday party ...   \n",
       "\n",
       "                                      Original Model  \\\n",
       "0        @Person1# wants to take a dictation for me.   \n",
       "1  #Person2# wants #Person2 to take a dictation f...   \n",
       "2              @Person1# is a memo to all employees.   \n",
       "3  #Person1# is stuck in traffic and a terrible t...   \n",
       "4                               You're finally here!   \n",
       "5  #Person2# is stuck in traffic and he wants to ...   \n",
       "6  You never believe what happened when Masha and...   \n",
       "7  #Person1# and Hero are getting divorced. #Pers...   \n",
       "8  @Person2# wants to divorce Masha and Hero. Mas...   \n",
       "9  #Person1# is a great party. #Person1# is happy to   \n",
       "\n",
       "                                          PEFT Model  \n",
       "0  #Person2# wants to take a dictation for #Person2#  \n",
       "1  #Person2# wants to take a dictation for #Person2#  \n",
       "2  #Person2# wants to take a dictation for #Person2#  \n",
       "3  #Person1# got stuck in traffic and got stuck i...  \n",
       "4  #Person1# got stuck in traffic and got stuck i...  \n",
       "5  #Person1# got stuck in traffic and got stuck i...  \n",
       "6  #Person1# is getting divorced. #Person2# is su...  \n",
       "7  #Person1# is getting divorced. #Person2# is su...  \n",
       "8  #Person1# is getting divorced. #Person2# is su...  \n",
       "9  #Person1# wishes Brian's birthday. #Person2# is a  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_baseline_summaries = []\n",
    "original_model_summaries = []\n",
    "peft_model_summaries = []\n",
    "\n",
    "for i in range(10):\n",
    "    human_baseline_summaries.append(dataset[\"test\"][i]['summary'])\n",
    "    prompt = f\"\"\"\n",
    "        Summarize the following conversation:\n",
    "        {dataset[\"test\"][i]['dialogue']}\n",
    "        Summary: \n",
    "\"\"\"\n",
    "    model_output = original_model.generate(tokenizer(prompt, return_tensors='pt')['input_ids'].to('xpu'))[0]\n",
    "    original_model_summaries.append(tokenizer.decode(model_output, skip_special_tokens=True))\n",
    "\n",
    "    peft_output = peft_model.generate(input_ids=tokenizer(prompt, return_tensors='pt')['input_ids'].to('xpu'))[0]\n",
    "    peft_model_summaries.append(tokenizer.decode(peft_output, skip_special_tokens=True))\n",
    "\n",
    "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, peft_model_summaries))\n",
    "df = pd.DataFrame(zipped_summaries, columns=['Human Baseline', 'Original Model', 'PEFT Model'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.25116375590640294, 'rouge2': 0.05666980405578849, 'rougeL': 0.20850201321975514, 'rougeLsum': 0.2032868898618424}\n",
      "{'rouge1': 0.2453671615740581, 'rouge2': 0.03356643356643356, 'rougeL': 0.21945933376967858, 'rougeLsum': 0.21751271716788956}\n"
     ]
    }
   ],
   "source": [
    "original_model_results = rouge.compute(\n",
    "    predictions=original_model_summaries, \n",
    "    references=human_baseline_summaries, \n",
    "    use_aggregator=True, \n",
    "    use_stemmer=True)\n",
    "\n",
    "peft_model_results = rouge.compute(\n",
    "    predictions=peft_model_summaries, \n",
    "    references=human_baseline_summaries, \n",
    "    use_aggregator=True, \n",
    "    use_stemmer=True)\n",
    "\n",
    "print(original_model_results)\n",
    "print(peft_model_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
