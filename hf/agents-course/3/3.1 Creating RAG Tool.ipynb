{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6263f73",
   "metadata": {},
   "source": [
    "# Building the Guestbook Tool\n",
    "\n",
    "## Step 1: Load and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065c4136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load the dataset\n",
    "guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")\n",
    "\n",
    "# Convert dataset entries into Document objects\n",
    "# docs = [\n",
    "#     Document(\n",
    "#         page_content=\"\\n\".join([\n",
    "#             f\"Name: {guest['name']}\",\n",
    "#             f\"Relation: {guest['relation']}\",\n",
    "#             f\"Description: {guest['description']}\",\n",
    "#             f\"Email: {guest['email']}\"\n",
    "#         ]),\n",
    "#         metadata={\"name\": guest[\"name\"]}\n",
    "#     )\n",
    "#     for guest in guest_dataset\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea732c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'name': 'Ada Lovelace'}, page_content=\"name: Ada Lovelace\\nrelation: best friend\\ndescription: Lady Ada Lovelace is my best friend. She is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine.\\nemail: ada.lovelace@example.com\\n\"), Document(metadata={'name': 'Dr. Nikola Tesla'}, page_content=\"name: Dr. Nikola Tesla\\nrelation: old friend from university days\\ndescription: Dr. Nikola Tesla is an old friend from your university days. He's recently patented a new wireless energy transmission system and would be delighted to discuss it with you. Just remember he's passionate about pigeons, so that might make for good small talk.\\nemail: nikola.tesla@gmail.com\\n\"), Document(metadata={'name': 'Marie Curie'}, page_content='name: Marie Curie\\nrelation: no relation\\ndescription: Marie Curie was a groundbreaking physicist and chemist, famous for her research on radioactivity.\\nemail: marie.curie@example.com\\n')]\n"
     ]
    }
   ],
   "source": [
    "docs = [Document(\n",
    "            page_content=\"\".join(f\"{key}: {value}\\n\" for key, value in guest.items()), \n",
    "            metadata={\"name\": guest[\"name\"]}\n",
    "        ) \n",
    "        for guest in guest_dataset]\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00be465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "splits = r_splitter.split_documents(docs)\n",
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad3fce83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Loading existing vector store from docs/chroma_guest/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IdeaPad\\AppData\\Local\\Temp\\ipykernel_18328\\1158825877.py:13: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "persist_directory = 'docs/chroma_guest/'\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Check if the vector store already exists\n",
    "if os.path.exists(persist_directory) and os.listdir(persist_directory):\n",
    "    print(f\"Loading existing vector store from {persist_directory}\")\n",
    "    # Load the existing vector store\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "else:\n",
    "    print(f\"Creating new vector store in {persist_directory}\")\n",
    "    # Create a new vector store from documents\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f13c52",
   "metadata": {},
   "source": [
    "## Step 2: Create the Retriever Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2923f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.tools import Tool\n",
    "\n",
    "# bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "\n",
    "# def extract_text(query: str) -> str:\n",
    "#     \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\"\n",
    "#     results = bm25_retriever.invoke(query)\n",
    "#     if results:\n",
    "#         return \"\\n\\n\".join([doc.page_content for doc in results[:3]])\n",
    "#     else:\n",
    "#         return \"No matching guest information found.\"\n",
    "    \n",
    "def extract_text(query: str) -> str:\n",
    "    \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\"\n",
    "    # results = bm25_retriever.invoke(query)\n",
    "    results = vectordb.similarity_search(query, k=3)\n",
    "    if results:\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "    else:\n",
    "        return \"No matching guest information found.\"\n",
    "\n",
    "guest_info_tool = Tool(\n",
    "    name=\"guest_info_retriever\",\n",
    "    func=extract_text,\n",
    "    description=\"Retrieves detailed information about gala guests based on their name or relation.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7814f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"name: Ada Lovelace\\nrelation: best friend\\n\\nemail: ada.lovelace@example.com\\n\\nrelation: best friend\\ndescription: Lady Ada Lovelace is my best friend. She is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_text(\"Tell me about the guest named Ada\")  # Example query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ca63e",
   "metadata": {},
   "source": [
    "### Tool to Access the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34d225db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "def extract_web(query: str) -> str:\n",
    "    \"\"\"Retrieves latest news about gala guests from the internet.\"\"\"\n",
    "    \n",
    "    search = DuckDuckGoSearchResults(output_format=\"list\", max_results=2)\n",
    "    results = search.run(query)\n",
    "    if results:\n",
    "        return results\n",
    "    else:\n",
    "        return \"No matching guest information found online.\"\n",
    "\n",
    "guest_info_web_tool = Tool(\n",
    "    name=\"guest_news_web_retriever\",\n",
    "    func=extract_web,\n",
    "    description=\"Retrieves latest news about gala guests from the internet.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc970cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_web(\"Tell me about Dr. Nikola Tesla\")  # Example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ba6e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "search_tool = TavilySearch(\n",
    "    max_results=2,\n",
    "    topic=\"general\",\n",
    "    # include_answer=False,\n",
    "    # include_raw_content=False,\n",
    "    # include_images=False,\n",
    "    # include_image_descriptions=False,\n",
    "    # search_depth=\"basic\",\n",
    "    time_range=\"week\",\n",
    "    # include_domains=None,\n",
    "    # exclude_domains=None\n",
    ")\n",
    "\n",
    "# from tavily import TavilyClient\n",
    "# def extract_web(query: str) -> str:\n",
    "#     \"\"\"Retrieves latest news about gala guests from the internet.\"\"\"\n",
    "    \n",
    "#     client = TavilyClient(os.environ[\"TAVILY_API_KEY\"])\n",
    "    \n",
    "#     results = client.search(\n",
    "#         query=query,\n",
    "#         max_results=2,\n",
    "#         time_range=\"week\"\n",
    "#     )\n",
    "#     if results:\n",
    "#         return results\n",
    "#     else:\n",
    "#         return \"No matching guest information found online.\"\n",
    "    \n",
    "# guest_info_web_tool = Tool(\n",
    "#     name=\"guest_news_web_retriever\",\n",
    "#     func=extract_web,\n",
    "#     description=\"Retrieves latest news about gala guests from the internet.\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1308c061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"Who's the current President of France?\",\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'title': 'Emmanuel Macron | Biography, Political Party, Age, Presidency, & Facts ...',\n",
       "   'url': 'https://www.britannica.com/biography/Emmanuel-Macron',\n",
       "   'content': \"Emmanuel Macron is a French banker and politician who was elected president of France in 2017. Macron was the first person in the history of the Fifth Republic to win the presidency without the backing of either the Socialists or the Gaullists, and he was France's youngest head of state since Napoleon.\",\n",
       "   'score': 0.7781275,\n",
       "   'raw_content': None},\n",
       "  {'title': 'Macron, Merz tout renewed Franco-German engine to lead Europe',\n",
       "   'url': 'https://www.politico.eu/article/france-macron-germany-merz-tout-shared-leadership-ukraine-competition-paris/',\n",
       "   'content': \"PARIS ‚Äî French President Emmanuel Macron and newly minted German Chancellor Friedrich Merz presented a shared vision for a well-armed and business-friendly Europe during their first formal meeting as peers on Wednesday. Fewer than 24 hours after formally becoming Germany's new leader, Merz traveled to Paris to meet with Macron at the Elys√©e Palace on how the so-called Franco-German engine\",\n",
       "   'score': 0.50915486,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 2.17}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tool.invoke(\"Who's the current President of France?\")\n",
    "# extract_web(\"Tell me about Dr. Nikola Tesla\")  # Example query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c4030",
   "metadata": {},
   "source": [
    "### Weather tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6114f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_weather_info(location: str) -> str:\n",
    "    \"\"\"Fetches dummy weather information for a given location.\"\"\"\n",
    "    # Dummy weather data\n",
    "    weather_conditions = [\n",
    "        {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
    "        {\"condition\": \"Clear\", \"temp_c\": 25},\n",
    "        {\"condition\": \"Windy\", \"temp_c\": 20}\n",
    "    ]\n",
    "    # Randomly select a weather condition\n",
    "    data = random.choice(weather_conditions)\n",
    "    return f\"Weather in {location}: {data['condition']}, {data['temp_c']}¬∞C\"\n",
    "\n",
    "# Initialize the tool\n",
    "weather_info_tool = Tool(\n",
    "    name=\"get_weather_info\",\n",
    "    func=get_weather_info,\n",
    "    description=\"Fetches dummy weather information for a given location.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e83960a",
   "metadata": {},
   "source": [
    "### Hub Stats Tool for Influential AI Builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a53e34fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IdeaPad\\AppData\\Local\\Temp\\ipykernel_18328\\4027241918.py:25: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  print(hub_stats_tool(\"facebook\")) # Example: Get the most downloaded model by Facebook\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most downloaded model by facebook is facebook/esmfold_v1 with 20,933,175 downloads.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import list_models\n",
    "\n",
    "def get_hub_stats(author: str) -> str:\n",
    "    \"\"\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\"\"\n",
    "    try:\n",
    "        # List models from the specified author, sorted by downloads\n",
    "        models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1))\n",
    "\n",
    "        if models:\n",
    "            model = models[0]\n",
    "            return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\"\n",
    "        else:\n",
    "            return f\"No models found for author {author}.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching models for {author}: {str(e)}\"\n",
    "\n",
    "# Initialize the tool\n",
    "hub_stats_tool = Tool(\n",
    "    name=\"get_hub_stats\",\n",
    "    func=get_hub_stats,\n",
    "    description=\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\n",
    ")\n",
    "\n",
    "# Example usage\n",
    "print(hub_stats_tool(\"facebook\")) # Example: Get the most downloaded model by Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992417b",
   "metadata": {},
   "source": [
    "## Step 3: Integrate the Tool with Alfred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0dec92e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tinit(init)\n",
      "\tassistant(assistant)\n",
      "\ttools(tools)\n",
      "\tcritique(critique)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> init;\n",
      "\tinit --> assistant;\n",
      "\ttools --> assistant;\n",
      "\tassistant -.-> tools;\n",
      "\tassistant -. &nbsp;__end__&nbsp; .-> critique;\n",
      "\tcritique -.-> init;\n",
      "\tcritique -.-> assistant;\n",
      "\tcritique -.-> tools;\n",
      "\tcritique -.-> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, Optional\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import uuid\n",
    "from IPython.display import Image, display, SVG\n",
    "import json\n",
    "\n",
    "llm_model = \"gemini-2.0-flash-lite\" # \"gemma-3-27b-it\" # \n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=llm_model,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "llm_critique = ChatGoogleGenerativeAI(\n",
    "    model=\"gemma-3-27b-it\",\n",
    "    temperature=0,\n",
    "    # max_tokens=None,\n",
    "    # timeout=None,\n",
    "    # max_retries=2,\n",
    ")\n",
    "\n",
    "tools = [guest_info_tool, search_tool, weather_info_tool, hub_stats_tool]\n",
    "chat_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Generate the AgentState and Agent graph\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    last_ai_message: Optional[str]\n",
    "    human_message: Optional[str]\n",
    "    critique_suggestion: Optional[str]\n",
    "    refine: Optional[bool]\n",
    "\n",
    "def init(state: AgentState):\n",
    "    last_human = None\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if last_human is None and isinstance(msg, HumanMessage):\n",
    "            last_human = msg\n",
    "            break\n",
    "    return {\n",
    "        \"human_message\": last_human.content,\n",
    "    }\n",
    "\n",
    "def assistant(state: AgentState):\n",
    "    return {\n",
    "        \"messages\": [chat_with_tools.invoke(state[\"messages\"])],\n",
    "    }\n",
    "\n",
    "\n",
    "def critique(state: AgentState):\n",
    "    last_ai = None\n",
    "\n",
    "    # Reverse loop to find the most recent ones\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if last_ai is None and isinstance(msg, AIMessage):\n",
    "            last_ai = msg\n",
    "            break\n",
    "    \n",
    "    state[\"last_ai_message\"] = last_ai.content\n",
    "\n",
    "    response = llm_critique.invoke(f\"Party Host: {state['human_message']}\\n\" \n",
    "                        + f\"Host Assistant: {state['last_ai_message']}\\n\"\n",
    "                        + \"Does Host's Assistant answer Party Host's question?\"\n",
    "                        + \"If no, has the assistant used the provided tools?\"\n",
    "                        + \"the provided tools are:\\n\"\n",
    "                        + \"\\n\".join([f\"- {tool.name}: {tool.description}\" for tool in tools])\n",
    "                        + \"Answer in JSON format:\\n\"\n",
    "                        + \"\"\"{\n",
    "                            \"answer\": True or False -> boolean,\n",
    "                            \"reason\": \"explain why\",\n",
    "                            \"suggestion\": \"suggest a possible improvement\"\n",
    "                            }\"\"\"\n",
    "    )\n",
    "    # Parse the response content safely\n",
    "    try:\n",
    "        json_str = response.content.strip()\n",
    "        # Optional: handle if response is wrapped in markdown/code block\n",
    "        if json_str.startswith(\"```json\"):\n",
    "            json_str = json_str[7:-3].strip()\n",
    "        result = json.loads(json_str)\n",
    "        \n",
    "        if not result.get(\"answer\", True):\n",
    "            state[\"critique_suggestion\"] = result.get(\"suggestion\", \"No suggestion provided.\")\n",
    "            state[\"messages\"].append(HumanMessage(content=f\"üõ†Ô∏è Suggested Improvement to your answer: {result.get(\"suggestion\", \"No suggestion provided.\")}\")),\n",
    "            refine = True\n",
    "        else:\n",
    "            refine= False\n",
    "    except Exception as e:\n",
    "        print(\"Critique parsing error:\", e)\n",
    "        refine = False\n",
    "        # return END  # Default to ending if JSON parsing fails\n",
    "    # if state.get(\"critique_suggestion\"):\n",
    "    #     suggestion = state[\"critique_suggestion\"]\n",
    "    #     state[\"messages\"].append(\n",
    "    #         HumanMessage(content=f\"üõ†Ô∏è Suggested Improvement: {suggestion}\")\n",
    "    #     )\n",
    "    #     # Clear it after use to avoid repeating\n",
    "    #     state[\"critique_suggestion\"] = None\n",
    "    # Could log here, or use for routing logic only\n",
    "    # return state\n",
    "    return {\n",
    "        \"refine\": refine,\n",
    "        # \"messages\": state[\"messages\"]\n",
    "        # no need to add last_ai_message or last_human_message if you don't need them later\n",
    "    }\n",
    "\n",
    "def critique_condition(state: AgentState):\n",
    "    if state[\"refine\"] == False:\n",
    "        # If the critique is not needed, end the conversation\n",
    "        return END\n",
    "    else:\n",
    "        # If the critique is needed, continue the conversation\n",
    "        return \"assistant\"\n",
    "\n",
    "\n",
    "## The graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"init\", init)\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "builder.add_node(\"critique\", critique)\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"init\")\n",
    "builder.add_edge(\"init\", \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message requires a tool, route to tools\n",
    "    # Otherwise, provide a direct response\n",
    "    tools_condition,\n",
    "    {\"tools\": \"tools\", \"__end__\": \"critique\"},\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"critique\",\n",
    "    critique_condition,\n",
    ")\n",
    "\n",
    "\n",
    "# Adding memory in langgraph!\n",
    "memory = MemorySaver()\n",
    "\n",
    "alfred = builder.compile(checkpointer=memory)\n",
    "# display(Image(alfred.get_graph(xray=True).draw_mermaid_png()))\n",
    "mermaid_code = alfred.get_graph(xray=True).draw_mermaid()\n",
    "print(mermaid_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cdc604d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé© Alfred's Response:\n",
      "Lady Ada Lovelace is my best friend. She is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine.\n"
     ]
    }
   ],
   "source": [
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are Alfred, a helpful assistant. You are hosting a gala dinner and need to answer questions about the guests or guest's question. You have access to guest database. You can also search the web for information. You have access to a weather tool and a Hugging Face Hub stats tool.\"),\n",
    "    HumanMessage(content=\"Tell me about our guest named 'Lady Ada Lovelace'.\")]\n",
    "response = alfred.invoke({\"messages\": messages}, config)\n",
    "messages = response[\"messages\"]  # üß† Carry memory forward\n",
    "\n",
    "print(\"üé© Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bcb8073e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé© Alfred's Response:\n",
      "You asked me to tell you about our guest named 'Lady Ada Lovelace'.\n"
     ]
    }
   ],
   "source": [
    "# Now let's ask a follow-up question\n",
    "messages.append(HumanMessage(content=\"What was my first question?\"))\n",
    "response = alfred.invoke({\"messages\": messages}, config,)\n",
    "messages = response[\"messages\"]\n",
    "\n",
    "print(\"üé© Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac37e488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé© Alfred's Response:\n",
      "Dr. Nikola Tesla is an old friend from your university days. He's recently patented a new wireless energy transmission system and would be delighted to discuss it with you. Just remember he's passionate about pigeons, so that might make for good small talk.\n"
     ]
    }
   ],
   "source": [
    "messages.append(HumanMessage(content=\"Who is Tesla?\"))\n",
    "response = alfred.invoke({\"messages\": messages}, config,)\n",
    "messages = response[\"messages\"]\n",
    "\n",
    "print(\"üé© Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d32e4b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé© Alfred's Response:\n",
      "Understood. I will use the `tavily_search` tool with the queries \"Who is Facebook?\" and \"What is Facebook's most popular model?\" and present the results to you.\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Who is Facebook and what's their most popular model?\")]\n",
    "response = alfred.invoke({\"messages\": messages}, config)\n",
    "\n",
    "print(\"üé© Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df926daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé© Alfred's Response:\n",
      "I am sorry, I cannot fulfill this request. I do not have access to information about Tesla.\n"
     ]
    }
   ],
   "source": [
    "messages.append(HumanMessage(content=\"Tell me the latest news about Tesla?\"))\n",
    "response = alfred.invoke({\"messages\": messages}, config,)\n",
    "messages = response[\"messages\"]\n",
    "\n",
    "print(\"üé© Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
