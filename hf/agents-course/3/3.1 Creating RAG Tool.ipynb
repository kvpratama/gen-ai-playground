{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6263f73",
   "metadata": {},
   "source": [
    "# Building the Guestbook Tool\n",
    "\n",
    "## Step 1: Load and Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065c4136",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# Load the dataset\n",
    "guest_dataset = datasets.load_dataset(\"agents-course/unit3-invitees\", split=\"train\")\n",
    "\n",
    "# Convert dataset entries into Document objects\n",
    "# docs = [\n",
    "#     Document(\n",
    "#         page_content=\"\\n\".join([\n",
    "#             f\"Name: {guest['name']}\",\n",
    "#             f\"Relation: {guest['relation']}\",\n",
    "#             f\"Description: {guest['description']}\",\n",
    "#             f\"Email: {guest['email']}\"\n",
    "#         ]),\n",
    "#         metadata={\"name\": guest[\"name\"]}\n",
    "#     )\n",
    "#     for guest in guest_dataset\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ea732c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'name': 'Ada Lovelace'}, page_content=\"name: Ada Lovelace\\nrelation: best friend\\ndescription: Lady Ada Lovelace is my best friend. She is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine.\\nemail: ada.lovelace@example.com\\n\"), Document(metadata={'name': 'Dr. Nikola Tesla'}, page_content=\"name: Dr. Nikola Tesla\\nrelation: old friend from university days\\ndescription: Dr. Nikola Tesla is an old friend from your university days. He's recently patented a new wireless energy transmission system and would be delighted to discuss it with you. Just remember he's passionate about pigeons, so that might make for good small talk.\\nemail: nikola.tesla@gmail.com\\n\"), Document(metadata={'name': 'Marie Curie'}, page_content='name: Marie Curie\\nrelation: no relation\\ndescription: Marie Curie was a groundbreaking physicist and chemist, famous for her research on radioactivity.\\nemail: marie.curie@example.com\\n')]\n"
     ]
    }
   ],
   "source": [
    "docs = [Document(\n",
    "            page_content=\"\".join(f\"{key}: {value}\\n\" for key, value in guest.items()), \n",
    "            metadata={\"name\": guest[\"name\"]}\n",
    "        ) \n",
    "        for guest in guest_dataset]\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d00be465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
    ")\n",
    "splits = r_splitter.split_documents(docs)\n",
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad3fce83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "Loading existing vector store from docs/chroma_guest/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\IdeaPad\\AppData\\Local\\Temp\\ipykernel_14880\\1158825877.py:13: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "persist_directory = 'docs/chroma_guest/'\n",
    "# embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Check if the vector store already exists\n",
    "if os.path.exists(persist_directory) and os.listdir(persist_directory):\n",
    "    print(f\"Loading existing vector store from {persist_directory}\")\n",
    "    # Load the existing vector store\n",
    "    vectordb = Chroma(\n",
    "        persist_directory=persist_directory,\n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "else:\n",
    "    print(f\"Creating new vector store in {persist_directory}\")\n",
    "    # Create a new vector store from documents\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f13c52",
   "metadata": {},
   "source": [
    "## Step 2: Create the Retriever Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2923f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.retrievers import BM25Retriever\n",
    "# from langchain.tools import Tool\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "# bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "\n",
    "# def extract_text(query: str) -> str:\n",
    "#     \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\"\n",
    "#     results = bm25_retriever.invoke(query)\n",
    "#     if results:\n",
    "#         return \"\\n\\n\".join([doc.page_content for doc in results[:3]])\n",
    "#     else:\n",
    "#         return \"No matching guest information found.\"\n",
    "@tool\n",
    "def extract_text(query: str) -> str:\n",
    "    \"\"\"Retrieves detailed information about gala guests based on their name or relation.\"\"\"\n",
    "    # results = bm25_retriever.invoke(query)\n",
    "    results = vectordb.similarity_search(query, k=3)\n",
    "    if results:\n",
    "        return \"\\n\\n\".join([doc.page_content for doc in results])\n",
    "    else:\n",
    "        return \"No matching guest information found.\"\n",
    "\n",
    "# guest_info_tool = Tool(\n",
    "    # name=\"guest_info_retriever\",\n",
    "    # func=extract_text,\n",
    "    # description=\"Retrieves detailed information about gala guests based on their name or relation.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb7814f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"name: Ada Lovelace\\nrelation: best friend\\n\\nemail: ada.lovelace@example.com\\n\\nrelation: best friend\\ndescription: Lady Ada Lovelace is my best friend. She is an esteemed mathematician and friend. She is renowned for her pioneering work in mathematics and computing, often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_text.invoke(\"Tell me about the guest named Ada\")  # Example query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74ca63e",
   "metadata": {},
   "source": [
    "### Tool to Access the internet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34d225db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults\n",
    "\n",
    "@tool\n",
    "def extract_web(query: str) -> str:\n",
    "    \"\"\"Retrieves latest news about gala guests from the internet.\"\"\"\n",
    "    \n",
    "    search = DuckDuckGoSearchResults(output_format=\"list\", max_results=2)\n",
    "    results = search.run(query)\n",
    "    if results:\n",
    "        return results\n",
    "    else:\n",
    "        return \"No matching guest information found online.\"\n",
    "\n",
    "# guest_info_web_tool = Tool(\n",
    "    # name=\"guest_news_web_retriever\",\n",
    "    # func=extract_web,\n",
    "    # description=\"Retrieves latest news about gala guests from the internet.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc970cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_web.invoke(\"Tell me about Dr. Nikola Tesla\")  # Example query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ba6e633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "\n",
    "search_tool = TavilySearch(\n",
    "    max_results=2,\n",
    "    topic=\"general\",\n",
    "    # include_answer=False,\n",
    "    # include_raw_content=False,\n",
    "    # include_images=False,\n",
    "    # include_image_descriptions=False,\n",
    "    # search_depth=\"basic\",\n",
    "    time_range=\"week\",\n",
    "    # include_domains=None,\n",
    "    # exclude_domains=None\n",
    ")\n",
    "\n",
    "# from tavily import TavilyClient\n",
    "# def extract_web(query: str) -> str:\n",
    "#     \"\"\"Retrieves latest news about gala guests from the internet.\"\"\"\n",
    "    \n",
    "#     client = TavilyClient(os.environ[\"TAVILY_API_KEY\"])\n",
    "    \n",
    "#     results = client.search(\n",
    "#         query=query,\n",
    "#         max_results=2,\n",
    "#         time_range=\"week\"\n",
    "#     )\n",
    "#     if results:\n",
    "#         return results\n",
    "#     else:\n",
    "#         return \"No matching guest information found online.\"\n",
    "    \n",
    "# guest_info_web_tool = Tool(\n",
    "#     name=\"guest_news_web_retriever\",\n",
    "#     func=extract_web,\n",
    "#     description=\"Retrieves latest news about gala guests from the internet.\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1308c061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': \"Who's the current President of France?\",\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'title': \"What is French President's two-hour TV interview about?\",\n",
       "   'url': 'https://www.connexionfrance.com/news/what-is-french-presidents-two-hour-tv-interview-about/723622',\n",
       "   'content': 'French President Emmanuel Macron is widely expected to make a return to domestic politics during a two-hour televised debate tonight (May 13). The president, who has focused on international issues such as the war in Ukraine and the election of US president Donald Trump in recent weeks, will be interviewed from 20:10 on the national TF1 channel.',\n",
       "   'score': 0.7570233,\n",
       "   'raw_content': None},\n",
       "  {'title': 'List of prime ministers of France - Wikipedia',\n",
       "   'url': 'https://en.wikipedia.org/wiki/List_of_prime_ministers_of_France',\n",
       "   'content': 'The head of the government of France has been called the prime minister of France (French: Premier ministre) since 1959, when Michel Debré became the first officeholder appointed under the Fifth Republic.During earlier periods of history, the head of government of France was known by different titles. As was common in European democracies of the 1815-1958 period (the Bourbon Restoration and',\n",
       "   'score': 0.24940656,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 2.25}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tool.invoke(\"Who's the current President of France?\")\n",
    "# extract_web(\"Tell me about Dr. Nikola Tesla\")  # Example query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c4030",
   "metadata": {},
   "source": [
    "### Weather tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6114f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "@tool\n",
    "def get_weather_info(location: str) -> str:\n",
    "    \"\"\"Fetches weather information for a given location.\"\"\"\n",
    "    # Dummy weather data\n",
    "    weather_conditions = [\n",
    "        {\"condition\": \"Rainy\", \"temp_c\": 15},\n",
    "        {\"condition\": \"Clear\", \"temp_c\": 25},\n",
    "        {\"condition\": \"Windy\", \"temp_c\": 20}\n",
    "    ]\n",
    "    # Randomly select a weather condition\n",
    "    data = random.choice(weather_conditions)\n",
    "    return f\"Weather in {location}: {data['condition']}, {data['temp_c']}°C\"\n",
    "\n",
    "# Initialize the tool\n",
    "# weather_info_tool = Tool(\n",
    "    # name=\"get_weather_info\",\n",
    "    # func=get_weather_info,\n",
    "    # description=\"Fetches dummy weather information for a given location.\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e83960a",
   "metadata": {},
   "source": [
    "### Hub Stats Tool for Influential AI Builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a53e34fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most downloaded model by facebook is facebook/esmfold_v1 with 21,000,492 downloads.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import list_models\n",
    "\n",
    "@tool\n",
    "def get_hub_stats(author: str) -> str:\n",
    "    \"\"\"Fetches the most downloaded/popular model from a given author on the Hugging Face Hub.\"\"\"\n",
    "    try:\n",
    "        # List models from the specified author, sorted by downloads\n",
    "        models = list(list_models(author=author, sort=\"downloads\", direction=-1, limit=1))\n",
    "\n",
    "        if models:\n",
    "            model = models[0]\n",
    "            return f\"The most downloaded model by {author} is {model.id} with {model.downloads:,} downloads.\"\n",
    "        else:\n",
    "            return f\"No models found for author {author}.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching models for {author}: {str(e)}\"\n",
    "\n",
    "# Initialize the tool\n",
    "# hub_stats_tool = Tool(\n",
    "    # name=\"get_hub_stats\",\n",
    "    # func=get_hub_stats,\n",
    "    # description=\"Fetches the most downloaded model from a specific author on the Hugging Face Hub.\"\n",
    "# )\n",
    "\n",
    "# Example usage\n",
    "print(get_hub_stats.invoke(\"facebook\")) # Example: Get the most downloaded model by Facebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c992417b",
   "metadata": {},
   "source": [
    "## Step 3: Integrate the Tool with Alfred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dec92e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "config:\n",
      "  flowchart:\n",
      "    curve: linear\n",
      "---\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tinit(init)\n",
      "\tassistant(assistant)\n",
      "\ttools(tools)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\t__start__ --> init;\n",
      "\tinit --> assistant;\n",
      "\ttools --> assistant;\n",
      "\tassistant -.-> tools;\n",
      "\tassistant -.-> __end__;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Annotated, Optional\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import AnyMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import uuid\n",
    "from IPython.display import Image, display\n",
    "import json\n",
    "\n",
    "llm_model = \"gemini-2.0-flash\" # \"gemma-3-27b-it\" # \"gemini-2.0-flash-lite\" # \n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=llm_model,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "llm_critique = ChatGoogleGenerativeAI(\n",
    "    model=\"gemma-3-27b-it\",\n",
    "    temperature=0,\n",
    "    # max_tokens=None,\n",
    "    # timeout=None,\n",
    "    # max_retries=2,\n",
    ")\n",
    "\n",
    "# tools = [guest_info_tool, search_tool, weather_info_tool, hub_stats_tool]\n",
    "tools = [extract_text, search_tool, get_weather_info, get_hub_stats]\n",
    "chat_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Generate the AgentState and Agent graph\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "    last_ai_message: Optional[str]\n",
    "    human_message: Optional[str]\n",
    "    critique_suggestion: Optional[str]\n",
    "    refine: Optional[bool]\n",
    "\n",
    "def init(state: AgentState):\n",
    "    last_human = None\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if last_human is None and isinstance(msg, HumanMessage):\n",
    "            last_human = msg\n",
    "            break\n",
    "    return {\n",
    "        \"human_message\": last_human.content,\n",
    "    }\n",
    "\n",
    "def assistant(state: AgentState):\n",
    "    return {\n",
    "        \"messages\": [chat_with_tools.invoke(state[\"messages\"])],\n",
    "    }\n",
    "\n",
    "\n",
    "def critique(state: AgentState):\n",
    "    last_ai = None\n",
    "\n",
    "    # Reverse loop to find the most recent ones\n",
    "    for msg in reversed(state[\"messages\"]):\n",
    "        if last_ai is None and isinstance(msg, AIMessage):\n",
    "            last_ai = msg\n",
    "            break\n",
    "    \n",
    "    state[\"last_ai_message\"] = last_ai.content\n",
    "\n",
    "    response = llm_critique.invoke(f\"Party Host: {state['human_message']}\\n\" \n",
    "                        + f\"Host Assistant: {state['last_ai_message']}\\n\"\n",
    "                        + \"Does Host's Assistant answer Party Host's question?\"\n",
    "                        + \"If no, can the assistant use the provided tools?\"\n",
    "                        + \"the provided tools are:\\n\"\n",
    "                        + \"\\n\".join([f\"- {tool.name}: {tool.description}\" for tool in tools])\n",
    "                        + \"Answer in JSON format:\\n\"\n",
    "                        + \"\"\"{\n",
    "                            \"answer\": True or False -> boolean,\n",
    "                            \"reason\": \"explain why\",\n",
    "                            \"suggestion\": \"suggest a possible improvement\"\n",
    "                            }\"\"\"\n",
    "    )\n",
    "    # Parse the response content safely\n",
    "    try:\n",
    "        json_str = response.content.strip()\n",
    "        # Optional: handle if response is wrapped in markdown/code block\n",
    "        if json_str.startswith(\"```json\"):\n",
    "            json_str = json_str[7:-3].strip()\n",
    "        result = json.loads(json_str)\n",
    "        \n",
    "        if not result.get(\"answer\", True):\n",
    "            state[\"critique_suggestion\"] = result.get(\"suggestion\", \"No suggestion provided.\")\n",
    "            state[\"messages\"].append(HumanMessage(content=f\"🛠️ Suggested Improvement to your answer: {result.get(\"suggestion\", \"No suggestion provided.\")}\")),\n",
    "            refine = True\n",
    "        else:\n",
    "            refine= False\n",
    "    except Exception as e:\n",
    "        print(\"Critique parsing error:\", e)\n",
    "        refine = False\n",
    "    return {\n",
    "        \"refine\": refine,\n",
    "    }\n",
    "\n",
    "def critique_condition(state: AgentState):\n",
    "    if state[\"refine\"] == False:\n",
    "        # If the critique is not needed, end the conversation\n",
    "        return END\n",
    "    else:\n",
    "        # If the critique is needed, continue the conversation\n",
    "        return \"assistant\"\n",
    "\n",
    "\n",
    "## The graph\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"init\", init)\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "# builder.add_node(\"critique\", critique)\n",
    "\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"init\")\n",
    "builder.add_edge(\"init\", \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    # If the latest message requires a tool, route to tools\n",
    "    # Otherwise, provide a direct response\n",
    "    tools_condition,\n",
    "    # {\"tools\": \"tools\", \"__end__\": \"critique\"},\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "# builder.add_conditional_edges(\n",
    "#     \"critique\",\n",
    "#     critique_condition,\n",
    "# )\n",
    "\n",
    "\n",
    "# Adding memory in langgraph!\n",
    "memory = MemorySaver()\n",
    "\n",
    "alfred = builder.compile(checkpointer=memory)\n",
    "# display(Image(alfred.get_graph(xray=True).draw_mermaid_png()))\n",
    "mermaid_code = alfred.get_graph(xray=True).draw_mermaid()\n",
    "print(mermaid_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cdc604d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎩 Alfred's Response:\n",
      "Ada Lovelace was a mathematician and is considered a best friend. She is renowned for her pioneering work in mathematics and computing, and is often celebrated as the first computer programmer due to her work on Charles Babbage's Analytical Engine. Her email is ada.lovelace@example.com.\n"
     ]
    }
   ],
   "source": [
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "messages = [\n",
    "    # SystemMessage(content=\"You are Alfred, a helpful assistant. You are hosting a gala dinner and need to answer questions about the guests or guest's question. You always answer the question objectively and not in first person. You have access to guest database. You can also search the web for information. You have access to a weather tool and a Hugging Face Hub stats tool to get information about author's model.Make sure to try various the tools before saying you don't know.\"),\n",
    "    HumanMessage(content=\"Tell me about 'Lady Ada Lovelace'.\")]\n",
    "response = alfred.invoke({\"messages\": messages}, config)\n",
    "messages = response[\"messages\"]  # 🧠 Carry memory forward\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1404ccb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎩 Alfred's Response:\n",
      "The weather in Paris is rainy and the temperature is 15°C. This weather is not suitable for a fireworks display.\n"
     ]
    }
   ],
   "source": [
    "response = alfred.invoke({\"messages\": \"What's the weather like in Paris tonight? Will it be suitable for our fireworks display?\"}, config)\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d32e4b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎩 Alfred's Response:\n",
      "I cannot directly provide information about Facebook using the available tools. However, I can search for information about Facebook using the tavily_search tool and for their most popular model on Hugging Face using the get_hub_stats tool, if you provide me with their Hugging Face author name. Would you like me to do that?\n"
     ]
    }
   ],
   "source": [
    "messages = [HumanMessage(content=\"Who is Facebook and what's their most popular model on Hugging Face?\")]\n",
    "response = alfred.invoke({\"messages\": messages}, config)\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bcb8073e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎩 Alfred's Response:\n",
      "Your first question was: \"Tell me about 'Lady Ada Lovelace'.\"\n"
     ]
    }
   ],
   "source": [
    "# Now let's ask a follow-up question\n",
    "messages.append(HumanMessage(content=\"What was my first question?\"))\n",
    "response = alfred.invoke({\"messages\": messages}, config,)\n",
    "messages = response[\"messages\"]\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac37e488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎩 Alfred's Response:\n",
      "Tesla is Dr. Nikola Tesla, an old friend from your university days. He recently patented a new wireless energy transmission system and would be delighted to discuss it with you. He's passionate about pigeons. His email is nikola.tesla@gmail.com.\n"
     ]
    }
   ],
   "source": [
    "messages.append(HumanMessage(content=\"Who is Tesla?\"))\n",
    "response = alfred.invoke({\"messages\": messages}, config,)\n",
    "messages = response[\"messages\"]\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df926daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎩 Alfred's Response:\n",
      "Here's the latest news about Tesla:\n",
      "\n",
      "*   Elon Musk confirmed that Tesla's Robotaxi platform would eventually come to Saudi Arabia.\n",
      "*   Musk shared a video of Tesla's humanoid robot, Optimus, dancing with improved flexibility and control.\n",
      "*   Tesla introduced a cheaper, rear-wheel drive option for the Model Y, signaling efforts to find new consumers. However, the refresh to the best-selling Model Y SUV starts on a rocky road.\n"
     ]
    }
   ],
   "source": [
    "messages.append(HumanMessage(content=\"Tell me the latest news about Tesla?\"))\n",
    "response = alfred.invoke({\"messages\": messages}, config)\n",
    "messages = response[\"messages\"]\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d2857418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎩 Alfred's Response:\n",
      "I need the author's Hugging Face Hub username to find their most popular model. Could you please provide that?\n"
     ]
    }
   ],
   "source": [
    "response = alfred.invoke({\"messages\": \"One of our guests is from Qwen. What can you tell me about their most popular model?\"}, config)\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2707a695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎩 Alfred's Response:\n",
      "Okay, here's some information to help you prepare for your conversation with Dr. Tesla about recent advancements in wireless energy:\n",
      "\n",
      "*   **Recent advancements in wideband rectennas for RF energy harvesting:** Research is focusing on design strategies, impedance matching, and efficiency enhancement.\n",
      "*   **Miniaturized Wireless Communication Systems:** There have been significant developments in mmWave technology for 5G networks and increased adoption of AI and machine learning in miniaturized wireless systems.\n",
      "\n",
      "Also, remember that Dr. Tesla is passionate about pigeons, so that might make for good small talk. Good luck with your conversation!\n"
     ]
    }
   ],
   "source": [
    "response = alfred.invoke({\"messages\":\"I need to speak with 'Dr. Nikola Tesla' about recent advancements in wireless energy. Can you help me prepare for this conversation?\"}, config)\n",
    "\n",
    "print(\"🎩 Alfred's Response:\")\n",
    "print(response['messages'][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
