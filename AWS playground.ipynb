{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\IdeaPad\\anaconda3\\envs\\pytorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 12460\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1500\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = datasets.load_dataset(hf_dataset_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 12460/12460 [00:00<00:00, 369435.10 examples/s]\n",
      "Filter: 100%|██████████| 500/500 [00:00<00:00, 19186.94 examples/s]\n",
      "Filter: 100%|██████████| 1500/1500 [00:00<00:00, 49901.69 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 125\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 5\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 15\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset = dataset.filter(lambda x, indexes: [index % 100 == 0 for index in indexes], with_indices=True, batched=True)\n",
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 12460/12460 [00:00<00:00, 163486.86 examples/s]\n",
      "Filter: 100%|██████████| 500/500 [00:00<00:00, 18512.01 examples/s]\n",
      "Filter: 100%|██████████| 1500/1500 [00:00<00:00, 41668.31 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 1244\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 50\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
       "        num_rows: 150\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filtered_dataset = dataset.filter(lambda xs, indexes: len(xs['dialogue']) > 200, with_indices=True, batched=False)\n",
    "filtered_dataset = dataset.filter(lambda xs, indexes: [len(x) > 200 and index%10 == 0 for x, index in zip(xs['dialogue'], indexes)], with_indices=True, batched=True)\n",
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
      "#Person2#: I found it would be a good idea to get a check-up.\n",
      "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
      "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n",
      "#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n",
      "#Person2#: Ok.\n",
      "#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n",
      "#Person2#: Yes.\n",
      "#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n",
      "#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n",
      "#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n",
      "#Person2#: Ok, thanks doctor.\n",
      "#PERSON1#: HI, MR. SMITH. I'M DOCTOR HAWKINS. WHY ARE YOU HERE TODAY?\n",
      "#PERSON2#: I FOUND IT WOULD BE A GOOD IDEA TO GET A CHECK-UP.\n",
      "#PERSON1#: YES, WELL, YOU HAVEN'T HAD ONE FOR 5 YEARS. YOU SHOULD HAVE ONE EVERY YEAR.\n",
      "#PERSON2#: I KNOW. I FIGURE AS LONG AS THERE IS NOTHING WRONG, WHY GO SEE THE DOCTOR?\n",
      "#PERSON1#: WELL, THE BEST WAY TO AVOID SERIOUS ILLNESSES IS TO FIND OUT ABOUT THEM EARLY. SO TRY TO COME AT LEAST ONCE A YEAR FOR YOUR OWN GOOD.\n",
      "#PERSON2#: OK.\n",
      "#PERSON1#: LET ME SEE HERE. YOUR EYES AND EARS LOOK FINE. TAKE A DEEP BREATH, PLEASE. DO YOU SMOKE, MR. SMITH?\n",
      "#PERSON2#: YES.\n",
      "#PERSON1#: SMOKING IS THE LEADING CAUSE OF LUNG CANCER AND HEART DISEASE, YOU KNOW. YOU REALLY SHOULD QUIT.\n",
      "#PERSON2#: I'VE TRIED HUNDREDS OF TIMES, BUT I JUST CAN'T SEEM TO KICK THE HABIT.\n",
      "#PERSON1#: WELL, WE HAVE CLASSES AND SOME MEDICATIONS THAT MIGHT HELP. I'LL GIVE YOU MORE INFORMATION BEFORE YOU LEAVE.\n",
      "#PERSON2#: OK, THANKS DOCTOR.\n"
     ]
    }
   ],
   "source": [
    "map_dataset = dataset.map(lambda xs: {\"dialogue_upper\": [x.upper() for x in xs['dialogue']]}, batched=True)\n",
    "print(map_dataset['train'][0]['dialogue'])\n",
    "print(map_dataset['train'][0]['dialogue_upper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Person1#: What do you want to know about me?\n",
      "#Person2#: How about your academic records at college?\n",
      "#Person1#: The average grade of all my courses is above 85.\n",
      "#Person2#: In which subject did you get the highest marks?\n",
      "#Person1#: In mathematics I got a 98.\n",
      "#Person2#: Have you received any scholarships?\n",
      "#Person1#: Yes, I have, and three times in total.\n",
      "#Person2#: Have you been a class leader?\n",
      "#Person1#: I have been a class commissary in charge of studies for two years.\n",
      "#Person2#: Did you join in any club activities?\n",
      "#Person1#: I was an aerobics team member in college.\n",
      "#Person2#: What sport are you good at?\n",
      "#Person1#: I am good at sprint and table tennis.\n",
      "#Person2#: You are excellent.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "#Person1#: What do you want to know about me?\n",
      "#Per\n",
      "#Person1#: What do you want to know about me?\n",
      "#Person2#: How about your academic records at college?\n",
      "#Person1#: The average grade of all my courses is above 85.\n",
      "#Person2#: In which subject did you get the highest marks?\n",
      "#Person1#: In mathematics I got a 98.\n",
      "#Person2#: Have you received any scholarships?\n",
      "#Person1#: Yes, I have, and three times in total.\n",
      "#Person2#: Have you been a class leader?\n",
      "#Person1#: I have been a class commissary in charge of studies for two years.\n",
      "#Person2#: Did you join in any club activities?\n",
      "#Person1#: I was an aerobics team member in college.\n",
      "#Person2#: What sport are you good at?\n",
      "#Person1#: I am good at sprint and table tennis.\n",
      "#Person2#: You are excellent.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "#Person1#: What do you want to know about me?\n",
      "#Per\n"
     ]
    }
   ],
   "source": [
    "new_dataset = dataset.map(lambda x: {'short': x['dialogue'][:50]})\n",
    "print(new_dataset['train'][200]['dialogue'])\n",
    "print('-'.join('-' for _ in range(50)))\n",
    "print(new_dataset['train'][200]['short'])\n",
    "\n",
    "new_dataset_batched = dataset.map(lambda xs: {'short': [x[:50] for x in xs['dialogue']]}, batched=True)\n",
    "print(new_dataset_batched['train'][200]['dialogue'])\n",
    "print('-'.join('-' for _ in range(50)))\n",
    "print(new_dataset_batched['train'][200]['short'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'short'],\n",
      "        num_rows: 1246\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'short'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'short'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'short'],\n",
      "        num_rows: 1246\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'short'],\n",
      "        num_rows: 50\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'short'],\n",
      "        num_rows: 150\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "new_dataset_filter = new_dataset.filter(lambda x, index: index % 10 == 0, with_indices=True)\n",
    "print(new_dataset_filter)\n",
    "\n",
    "new_dataset_filter_batched = new_dataset.filter(lambda xs, indexes: [index % 10 == 0 for index in indexes], with_indices=True, batched=True)\n",
    "print(new_dataset_filter_batched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(lambda x: {'dialogue_tok': tokenizer(x['dialogue'])})\n",
    "print(tokenized_dataset)\n",
    "# print(tokenizer.decode(tokenized_dataset['train'][0]['dialogue_tok']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1713, 345, 13515, 536, 4663, 10, 2018, 6, 1363, 5, 3931, 5, 27, 31, 51, 7582, 12833, 77, 7, 5, 1615, 33, 25, 270, 469, 58, 1713, 345, 13515, 357, 4663, 10, 27, 435, 34, 133, 36, 3, 9, 207, 800, 12, 129, 3, 9, 691, 18, 413, 5, 1713, 345, 13515, 536, 4663, 10, 2163, 6, 168, 6, 25, 43, 29, 31, 17, 141, 80, 21, 305, 203, 5, 148, 225, 43, 80, 334, 215, 5, 1713, 345, 13515, 357, 4663, 10, 27, 214, 5, 27, 2320, 38, 307, 38, 132, 19, 1327, 1786, 6, 572, 281, 217, 8, 2472, 58, 1713, 345, 13515, 536, 4663, 10, 1548, 6, 8, 200, 194, 12, 1792, 2261, 21154, 19, 12, 253, 91, 81, 135, 778, 5, 264, 653, 12, 369, 44, 709, 728, 3, 9, 215, 21, 39, 293, 207, 5, 1713, 345, 13515, 357, 4663, 10, 8872, 5, 1713, 345, 13515, 536, 4663, 10, 1563, 140, 217, 270, 5, 696, 2053, 11, 11581, 320, 1399, 5, 2321, 3, 9, 1659, 6522, 6, 754, 5, 531, 25, 7269, 6, 1363, 5, 3931, 58, 1713, 345, 13515, 357, 4663, 10, 2163, 5, 1713, 345, 13515, 536, 4663, 10, 14627, 53, 19, 8, 1374, 1137, 13, 5084, 1874, 11, 842, 1994, 6, 25, 214, 5, 148, 310, 225, 10399, 5, 1713, 345, 13515, 357, 4663, 10, 27, 31, 162, 1971, 3986, 13, 648, 6, 68, 27, 131, 54, 31, 17, 1727, 12, 4583, 8, 7386, 5, 1713, 345, 13515, 536, 4663, 10, 1548, 6, 62, 43, 2287, 11, 128, 11208, 24, 429, 199, 5, 27, 31, 195, 428, 25, 72, 251, 274, 25, 1175, 5, 1713, 345, 13515, 357, 4663, 10, 8872, 6, 2049, 2472, 5, 1]\n",
      "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today? #Person2#: I found it would be a good idea to get a check-up. #Person1#: Yes, well, you haven't had one for 5 years. You should have one every year. #Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor? #Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good. #Person2#: Ok. #Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith? #Person2#: Yes. #Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit. #Person2#: I've tried hundreds of times, but I just can't seem to kick the habit. #Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave. #Person2#: Ok, thanks doctor.</s>\n",
      "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today?\n",
      "#Person2#: I found it would be a good idea to get a check-up.\n",
      "#Person1#: Yes, well, you haven't had one for 5 years. You should have one every year.\n",
      "#Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor?\n",
      "#Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good.\n",
      "#Person2#: Ok.\n",
      "#Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith?\n",
      "#Person2#: Yes.\n",
      "#Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit.\n",
      "#Person2#: I've tried hundreds of times, but I just can't seem to kick the habit.\n",
      "#Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave.\n",
      "#Person2#: Ok, thanks doctor.\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset['train'][0]['dialogue_tok']['input_ids'])\n",
    "print(tokenizer.decode(tokenized_dataset['train'][0]['dialogue_tok']['input_ids']))\n",
    "print(tokenized_dataset['train'][0]['dialogue'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 12460/12460 [00:15<00:00, 817.93 examples/s]\n",
      "Filter: 100%|██████████| 500/500 [00:00<00:00, 895.37 examples/s]\n",
      "Filter: 100%|██████████| 1500/1500 [00:01<00:00, 779.69 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 10845\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 425\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 1302\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_filter = tokenized_dataset.filter(lambda x: x['dialogue'].replace('\\n', ' ') == tokenizer.decode(x['dialogue_tok']['input_ids'], skip_special_tokens=True))\n",
    "\n",
    "print(dataset_filter)\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 12460/12460 [00:06<00:00, 1979.52 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 1769.92 examples/s]\n",
      "Map: 100%|██████████| 1500/1500 [00:00<00:00, 2168.97 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tok_dataset = dataset.map(lambda xs: {\"dialogue_tok\": [tokenizer(x) for x in xs['dialogue']]}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n",
      "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today? #Person2#: I found it would be a good idea to get a check-up. #Person1#: Yes, well, you haven't had one for 5 years. You should have one every year. #Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor? #Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good. #Person2#: Ok. #Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith? #Person2#: Yes. #Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit. #Person2#: I've tried hundreds of times, but I just can't seem to kick the habit. #Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave. #Person2#: Ok, thanks doctor.\n",
      "#Person1#: Hi, Mr. Smith. I'm Doctor Hawkins. Why are you here today? #Person2#: I found it would be a good idea to get a check-up. #Person1#: Yes, well, you haven't had one for 5 years. You should have one every year. #Person2#: I know. I figure as long as there is nothing wrong, why go see the doctor? #Person1#: Well, the best way to avoid serious illnesses is to find out about them early. So try to come at least once a year for your own good. #Person2#: Ok. #Person1#: Let me see here. Your eyes and ears look fine. Take a deep breath, please. Do you smoke, Mr. Smith? #Person2#: Yes. #Person1#: Smoking is the leading cause of lung cancer and heart disease, you know. You really should quit. #Person2#: I've tried hundreds of times, but I just can't seem to kick the habit. #Person1#: Well, we have classes and some medications that might help. I'll give you more information before you leave. #Person2#: Ok, thanks doctor.</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tok_dataset)\n",
    "print(dataset['train'][0]['dialogue'].replace('\\n', ' '))\n",
    "print(tokenizer.decode(tok_dataset['train'][0]['dialogue_tok']['input_ids']))\n",
    "dataset['train'][0]['dialogue'].replace('\\n', ' ') == tokenizer.decode(tok_dataset['train'][0]['dialogue_tok']['input_ids'], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter:   0%|          | 0/12460 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 12460/12460 [00:15<00:00, 783.91 examples/s]\n",
      "Filter: 100%|██████████| 500/500 [00:00<00:00, 898.93 examples/s]\n",
      "Filter: 100%|██████████| 1500/1500 [00:02<00:00, 743.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tok_dataset_filter = tok_dataset.filter(lambda xs: [x.replace('\\n', ' ') != tokenizer.decode(xt['input_ids'], skip_special_tokens=True) for x, xt in zip(xs['dialogue'], xs['dialogue_tok'])], batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 1615\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 75\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 198\n",
      "    })\n",
      "})\n",
      "#Person1#: Why didn't you tell me you had a girlfriend? \n",
      "#Person2#: Sorry, I thought you knew. \n",
      "#Person1#: But you should have told me you were in love with her. \n",
      "#Person2#: Didn't I? \n",
      "#Person1#: You know you didn't. \n",
      "#Person2#: Well, I'm telling you now. \n",
      "#Person1#: Yes, but you might have told me before. \n",
      "#Person2#: I didn't think you'd be interested. \n",
      "#Person1#: You can't be serious. How dare you not tell me you were going to marry her? \n",
      "#Person2#: Sorry, I didn't think it mattered. \n",
      "#Person1#: Oh, you men! You're all the same. \n",
      "#Person1#: Why didn't you tell me you had a girlfriend?  #Person2#: Sorry, I thought you knew.  #Person1#: But you should have told me you were in love with her.  #Person2#: Didn't I?  #Person1#: You know you didn't.  #Person2#: Well, I'm telling you now.  #Person1#: Yes, but you might have told me before.  #Person2#: I didn't think you'd be interested.  #Person1#: You can't be serious. How dare you not tell me you were going to marry her?  #Person2#: Sorry, I didn't think it mattered.  #Person1#: Oh, you men! You're all the same. \n",
      "#Person1#: Why didn't you tell me you had a girlfriend? #Person2#: Sorry, I thought you knew. #Person1#: But you should have told me you were in love with her. #Person2#: Didn't I? #Person1#: You know you didn't. #Person2#: Well, I'm telling you now. #Person1#: Yes, but you might have told me before. #Person2#: I didn't think you'd be interested. #Person1#: You can't be serious. How dare you not tell me you were going to marry her? #Person2#: Sorry, I didn't think it mattered. #Person1#: Oh, you men! You're all the same. \n"
     ]
    }
   ],
   "source": [
    "print(tok_dataset_filter)\n",
    "print(tok_dataset_filter['train'][0]['dialogue'])\n",
    "print(tok_dataset_filter['train'][0]['dialogue'].replace('\\n', ' '))\n",
    "print(tokenizer.decode(tok_dataset_filter['train'][0]['dialogue_tok']['input_ids'], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 10845\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 425\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic', 'dialogue_tok'],\n",
      "        num_rows: 1302\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tok_dataset)\n",
    "print(tok_dataset_filter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
